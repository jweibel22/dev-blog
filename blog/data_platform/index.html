<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>How to reach production in less than 6 months</title>
		<meta name="description" content="A collection of ideas on how to get from scratch to an MVP in the shortest possible time">
		<link rel="alternate" href="/dev-blog/feed/feed.xml" type="application/atom+xml" title="Developer Blog">
		<link rel="alternate" href="/dev-blog/feed/feed.json" type="application/json" title="Developer Blog">
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
h1, h2, h3 {
	color: #a07744;
  }

code {
    color: #E69C19;
    padding: 2px 4px;
}
* { box-sizing: border-box; }
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 60em;
}

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}
header:after {
	content: "";
	display: table;
	clear: both;
}

.links-nextprev {
	list-style: none;
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
}
code {
	word-break: break-all;
}

/* Header */
header {
	display: flex;
	gap: 1em .5em;
	flex-wrap: wrap;
	align-items: center;
	padding: 1em;
}
.home-link {
	font-size: 1em; /* 16px /16 */
	font-weight: 700;
	margin-right: 2em;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	margin-bottom: 10px;
	margin-top: 10px;
	word-spacing: -0.5px;
	flex-basis: calc(100% - 1.5rem);
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}

/* Direct Links / Markdown Headers */
.header-anchor {
	text-decoration: none;
	font-style: normal;
	font-size: 1em;
	margin-left: .1em;
}
a[href].header-anchor,
a[href].header-anchor:visited {
	color: transparent;
}
a[href].header-anchor:focus,
a[href].header-anchor:hover {
	text-decoration: underline;
}
a[href].header-anchor:focus,
:hover > a[href].header-anchor {
	color: #aaa;
}

h2 + .header-anchor {
	font-size: 1.5em;
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/dev-blog/" class="home-link">Developer Blog</a>
			<nav>
				<h2 class="visually-hidden">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/dev-blog/">Home</a></li>
					<li class="nav-item"><a href="/dev-blog/blog/">Archive</a></li>
					<li class="nav-item"><a href="/dev-blog/about/">About Me</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			
<h1>How to reach production in less than 6 months</h1>

<ul class="post-metadata">
	<li><time datetime="2024-08-02">02 August 2024</time></li>
	<li><a href="/dev-blog/tags/data/" class="post-tag">Data</a>, </li>
	<li><a href="/dev-blog/tags/developer-productivity/" class="post-tag">Developer productivity</a></li>
</ul>

<p>Some projects will have deadlines imposed by the business. They could originate from time-to-market considerations or from specific contract obligations, and for those projects delivery time is obviously important. However, I would ague that even on projects that have no hard deadlines, the time to reach production is still vital. Reaching production sooner rather than later builds trust with stakeholders and the developers will get a feeling that they're making an impact. It also closes the feedback loop. Getting feedback from experiments or test users from production is often the most valuable type of feedback and will help the agility of the project going forward.</p>
<p>In this post I share my experiences related to reducing the time to reach production on greenfield projects. In particular I will be referring to my experiences with building a data platform from scratch. It was a big undertaking, I was a one man army, and it was therefore absolutely vital to be pragmatic and diligent about prioritisation to ensure that the product could provide value to the business in a reasonable amount of time.</p>
<h2 id="what-is-the-mvp" tabindex="-1">What is the MVP <a class="header-anchor" href="#what-is-the-mvp">#</a></h2>
<p>Obviously you'll need to define the minimal viable project. What is the smallest set of requirements that you can deliver while still having a viable value proposition? If your product is replacing an existing product you must find out what capabilities that product has and you must take care to deliver a better alternative, or at least not something that is significantly worse. If the existing product is comprehensive you could consider replacing only parts of it, and then carry out the sunsetting in iterations. This will of course need to be a collaboration between the development team and stakeholders and/or product owners. If you're building a platform product it could be feasible to set up regular feedback sessions with your target users to help prioritise and shape the MVP.</p>
<p></p><div style="background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;">
<span>
<p style="margin-top:1em; text-align:center"><b></b></p>
<div style="margin:2em"><p></p>
<p>The company I was working with already had an existing product. The product consisted of a postgreql database, a BI tool, and a microservice that was subscribing to event data coming from other services from the backend and mapping the data into fact and dimension tables in the postgresql database.</p>
<p>The company was a startup and the initial motivation behind building the existing product was to make it possible for people from the business to keep track of various KPIs via the BI tool. The set of KPIs were fairly static so the solution worked ok for a while, but it didn't take long before other analytical use cases appeared. Those new use cases came with the need to get more data into the database and to execute queries that did not fit the facts and dimensions structure that had been built and did not utilize the indexes. Making these sorts of changes was done in the microservice. It was responsible for ingesting, cleaning and mapping the source data to the table structure in the database, and so to add a new data source a go developer was needed to add a subscription to the new event and create the needed mappings. This was painful, the go developer was not personally invested in the analytical use cases and this felt like a chore to him, and the analyst was stuck waiting for the go developer to prepare the tables. To make matters worse, most often it was impossible to backfill data, so when a new subscription to an event was created it was not possible to get hold of all the historical data that had been published before the subscription was created. This meant in practice that for data to be available for an analyst it would be required that he was able to predict which types of analytical queries he would be interested in making at the time when a new event started to be published from the backend. New analytical use cases pop up all the time so this is clearly not feasible.</p>
<p>What the company needed was a data warehouse that supported OLAP queries without requiring up front indexing. Such a data warehouse could obviously also easily solve the existing use case of making KPIs available to the business. Alleviating the pain associated with adding new data sources and supporting new queries and also solving the performance issues they were facing due to the postgresql database struggling to keep up with the large amounts of data was a very appealing value proposition and something &quot;worth waiting for&quot;. However, the existing product had not taken years to build, so it would not be reasonable to spend years building a data platform without delivering a product that could provide some value.</p>
<p>The absolute minimal set of requirements were determined to be:</p>
<ol>
<li>A data warehouse.</li>
<li>An ingestion pipeline that ingests the data from the backend into the data warehouse and makes it available for analysts.</li>
<li>Tooling that will make it possible to refine the raw event data in the data warehouse into better suited table structures. The tooling must be SQL based so analysts can own this.</li>
</ol>
<p>The initial product did not need to make ALL data from the backend available in the data warehouse, only the data that was currently in use was necessary. The SQL tooling did not need to be perfect, it just needed to be less painful than the existing solution, which was a very low bar.</p>
<p></p></div>
<p></p>
</span>
</div><p></p>
<h2 id="compromise" tabindex="-1">Compromise <a class="header-anchor" href="#compromise">#</a></h2>
<p>You will need to cut corners. Identify what are need-to-haves and what you can be less diligent about. You probably want to ensure durability and avoid loosing data, but perhaps the UX/DX doesn't need to be perfect?</p>
<p>You'll need to take calculated risks. E.g. if you release a component without testing it, what would be the impact to the users if a bug creeped into production, how would you resolve the issue and how long would it take to resolve it? It can be ok to fail in production if the impact is small and the time to recover is low, but if the production issues result in a large maintenance burden the ROI of implementing it and testing it properly before releasing it could be above 0.</p>
<p>The compromises you make will result in feature-incompleteness or perhaps a less reliable product. If you keep track of user/developer pains you'll be aware of these shortcomings and eventually they will get prioritised. However, you will feel a sting as it will conflict with your professional pride.</p>
<p></p><div style="background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;">
<span>
<p style="margin-top:1em; text-align:center"><b></b></p>
<div style="margin:2em"><p></p>
<p>We chose a tool called DBT for providing SQL based transformation capabilities to the users of the data platform. DBT was in the early stages at the time (2019), there was no dbt cloud and so we had to roll our own execution environment. We did this by copying the user code into a docker image containing a dbt installation as part of our build pipeline, and then we would run this image on kubernetes (using airflow as a scheduler). The docker image was very simple, it contained an entrypoint.sh shell script that included a <code>dbt run</code> command.</p>
<p>As time passed new requirements meant that the simple <code>dbt run</code> command needed to be extended with more options and we did this by extending the shell script. There were no tests of the shell script and sometimes bugs would sneak into the script causing the execution of dbt to fail in airflow. This was obviously an inconvenience to users who more often than not had no clue what was causing the error to occur. Most often the error was detected and corrected in a few minutes but it was still causing developer pains and a great source of instability, yet it was allowed to stay this way for 6 months.</p>
<p>Eventually we finally got around to making this more robust. The shell script was converted to python code and acceptance tests were added.</p>
<p></p></div>
<p></p>
</span>
</div><p></p>
<h2 id="ops" tabindex="-1">Ops <a class="header-anchor" href="#ops">#</a></h2>
<p>Going to production as early as possible entails having to operate immature software. This will most likely result in many production issues and a lot of related &quot;run&quot; work, and you'll need to have a plan for how to handle this. If you end up spending all your time doing run work the project will eventually stall.</p>
<p></p><div style="background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;">
<span>
<p style="margin-top:1em; text-align:center"><b></b></p>
<div style="margin:2em"><p></p>
<p>One of the classical problems faced by data platforms is how to get hold of the schemas of the data that is being ingested. The data that was ingested was based on rabbitmq messages exchanged between the backend services written in go and the schema definitions were written in go code which was buried inside of a larger shared go project and since the data platform was not written in go, it was written in scala and python, the schemas were not directly accessible.</p>
<p>In a situation where schema definitions are not readily available getting hold of the schema definitions will require manual effort, and one must decide where this responsibility lies. Does the responsibility lie with the data producers or with the consumers, in this case the data platform? Generally, this responsibility should lie with the data producers because if producers are required to provide a valid schema it allows anyone to consume the data without hassle, but in this case all consumers except for the data platform were go microservices and were therefore able to use the schemas provided in the go project. Moving to a language agnostic schema format, would require a great deal of discussions, alignments and agreements across the entire organisation. At the time, there was no formal RFC process in place and the data platform team had no mandate to make such requirements. Changing the schema definition process would require a lot of work for all backend teams and there was simply no appetite to prioritise this as the pain was only felt by the data teams. For this reason the responsibility of getting hold of the schemas fell on the data platform.</p>
<p>One popular solution in these situations is to extract schemas using schema inference. With schema inference the schemas in the platform are automatically inferred and dynamically adapted to the data as it is being ingested. Schema inference causes a lot of headaches, especially if many heterogenous (polyglot) sources are at play. E.g. Is “7&quot; an integer or a decimal number? (some languages/frameworks leaves out the decimal points when it is 0). Is “.” a decimal point or a thousands separator? These imperfections of schema inference inevitably results in inaccurate schemas and eventually in data that cannot be ingested because of mismatching schemas. Another common problem is that schemas can evolve in a non compatible way causing errors to occur when the schema inference process tries to register the new version of the schema.</p>
<p>Instead we chose to build tooling that could extract avro schemas from the aforementioned go project. The schemas in the go code were not defined in a declarative way, i.e. it was not as easy as scanning for structs and using the fully qualified name of the struct as the schema name. It was, however, possible to parse the go code of the project, traversed the AST and then by making a number of assumptions on the structure of the code extract the information that was needed from the code and generate the schemas in avro format. Luckily, it turned out that there was a lot of conformity among the developers who all adhered to the existing code structure when extending the code, which meant that the solution wasn’t as brittle as one might have feared. Nevertheless, because of the nature of how we had to extract the schemas and because we wanted to allow developer teams to experiment in dev/prod which might entail introducing breaking changes to schemas, there was a high probability that issues related to faulty schemas could cause ingestion of messages to halt and create noise in the logs that would require the attention of the engineers responsible for the platform and in many cases this work would be <code>urgent + not important</code> because the errors would be related to data that was not in use by the data analysts. We therefore chose not to automate the schema registration process but instead create a CLI that allowed a data analyst to generate and register a schema on demand using the tooling described above.</p>
<p>Because we needed to capture all the messages that were exchanged between the backend services and we required schemas to be defined upfront we needed to allow for schemaless data to exist in the data platform. To this end we decided to create two data lakes. An unstructured data lake and a structured data lake. The purpose of the unstructured data lake was to capture everything. It would contain all incoming data in its raw format (json in this case) without requiring a schema to be present. The structured data lake would contain the subset of data that had a schema defined.</p>
<p>Any issues related to schemas, e.g. issues with generating schemas from the go code, or schemas not being compatible with the ingested data, would now either occur in the tooling or in the data pipeline that applied the schema from the raw data. It would not effect the data pipeline that ingested the raw data into the unstructured data lake. I.e. the most critical requirement, not loosing any data, would be uncoupled from the inherent unstable process of applying schemas to the data, and thus those errors which we were expecting to see a lot of, especially in the beginning, were now no longer urgent and critical errors. Furthermore if schema related issues arose, users of the data platform could fix the invalid schema and run a backfill operation to correct the wrong data in the structured data lake, and all of this could be done without the presence of a member of the data platform team. Building two data lakes and two data pipelines rather than one is a larger effort, but the expectation was that the amount of avoided run work would quickly make up for this investment. (There are also many other advantages to making this split in the data lake that we will not get into in this post.)</p>
<p></p></div>
<p></p>
</span>
</div><p></p>
<p>See <a href="/dev-blog/blog/fail_fast/">Ops driven design</a> for a more general discussion on this topic.</p>
<h2 id="single-responsibility" tabindex="-1">Single responsibility <a class="header-anchor" href="#single-responsibility">#</a></h2>
<p>As mentioned, it might make sense to consider cutting corners in order to reach production in a reasonable amount of time. Cutting corners will have consequences, such as less stability or usability of the product, which can be a reasonable compromise to make in certain situations. I would, however, advise against violating the single responsibility principle. It can be very tempting to cram together multiple concerns into a single service, tool or CLI. It is, after all, faster to build a single service and a single API rather than multiple ones and there will be less movable parts.</p>
<p>However, you should consider the consequences of doing so. If the idea is to move fast and reach production in the shortest possible time, then when you reach production you'll have an unfinished product that you will need to develop further. When you violate the single responsibility principle it will be more difficult to evolve the product because different concerns, that you'll most likely want to evolve independently, are entangled in the architecture. This will hinder further development as you'll often find you'll need to refactor a component before you can evolve it. Reaching production fast only to slow down significantly later is not a good strategy.</p>
<p>Instead try to understand your problem space and build up your solution based on components that each serve a specific purpose. That way, when the product needs to be evolved, the individual components can be replaced or extended independently.</p>
<p></p><div style="background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;">
<span>
<p style="margin-top:1em; text-align:center"><b></b></p>
<div style="margin:2em"><p></p>
<p>As part of onboarding a new user on the data platform we needed to set up her development environment which included a personal database on a redshift  cluster. This personal database was used to run dbt locally as part of the development work flow (when developing new data models using dbt).
Setting this up involved executing some DDL statements on the redshift cluster for setting up the database, the user and permissions, and it would be necessary to repeat this if we wanted to provision a new redshift cluster, e.g. to increase capacity. We therefore wanted to automate this.</p>
<p>To comply with EU GDPR regulations it was important to limit access to PII (personally identifiable information). To this end we wanted to implement an RBAC mechanism, redshift did not support this at the time, where users would need to take on purpose specific roles in order to gain access to tables containing PII. To allow users to manage roles and permissions for those roles we needed a service that could take such a configuration and execute the required DDL statements to set up the users and permissions on the redshift database. This was a very similar problem to the problem described above of setting up personal databases.</p>
<p>We decided to implement a single service to handle both use cases. Users would maintain a single configuration file (protected by the 4 eyed principle) listing all users, all roles, and which users had access to which roles, and a backend service (a kubernetes controller to be exact) would apply the changes to the redshift clusters.</p>
<p>Now let's say we wanted to migrate to dbt cloud. All the logic related to setting up the development environment would need to be removed from the service, one would need to take care not to inadvertently change the rest of the logic. Similarly, let's say redshift implemented support for RBAC, we would no longer be required to maintain our own solution. One would need to remove that part from the service and again take great care not to touch the code that set up the development environment. Had each use case been implemented by separate services, or at least clearly separated in the service, evolving the product would have been easier.</p>
<p></p></div>
<p></p>
</span>
</div><p></p>

<ul class="links-nextprev"><li>Previous: <a href="/dev-blog/blog/reducing_run/">How to run faster</a></li><li>Next: <a href="/dev-blog/blog/sagas/">Reasons to use a saga library</a></li>
</ul>

		</main>

		<footer></footer>

		<!-- This page `/dev-blog/blog/data_platform/` was built on 2024-10-04T05:41:30.511Z -->
	</body>
</html>
