{
	"version": "https://jsonfeed.org/version/1.1",
	"title": "Developer Blog",
	"language": "en",
	"home_page_url": "https://example.com/dev-blog/",
	"feed_url": "https://example.com/dev-blog/feed/feed.json",
	"description": "I am writing about my experiences as a naval navel-gazer.",
	"author": {
		"name": "Jimmy Weibel Rasmussen",
		"url": "https://example.com/about-me/"
	},
	"items": [
		{
			"id": "https://example.com/dev-blog/blog/sagas/",
			"url": "https://example.com/dev-blog/blog/sagas/",
			"title": "Reasons to use a saga library",
			"content_html": "<p>When building a backend using the microservices architecture you'll most likely find yourself in a situation where you need to execute a transaction or a business workflow that spans multiple services. Since each service has its own data store maintaining consistency is non trivial, if one of the individual transactions that make up the distributed transaction fails it is necessary to roll back everything to get back to a consistent state. A few data stores provides distributed ACID transactions via 2PC (two phase commits) but 2PC are notoriously difficult to work with, breaks service encapsulation and can have a serious impact on performance due to lock congestion. For these reasons it is common practice to embrace eventual consistency and implement distributed transactions using the saga pattern.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Terminology</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>In the definition of sagas used by <a href=\"https://example.com/dev-blog/blog/sagas/(https:/microservices.io/patterns/data/saga.html)\">Chris Richardson</a> (and others) sagas can either be orchestrated or choreographed. The implementation of a choreographed saga is distributed amongst the participating services, each service is subscribing to a set of events and will reactively execute side effects and possibly publish new events to the event bus that will move the process forward. This is an event driven approach that fosters a loose coupling between the services. Conversely an orchestrated saga is implemented by a single service that has the responsibility of keeping track of the state of the process, making calls to other services in a sequence of steps. This can result in a more tightly coupled architecture but the fact that the responsibility of the flow is anchored inside a single service makes it easier to debug and operate the sagas. Both approaches have their merits but this particular post is about orchestrating sagas.</p>\n<p>Sagas are often referred to as <a href=\"https://learn.microsoft.com/en-us/previous-versions/msp-n-p/jj591569(v=pandp.10)?redirectedfrom=MSDN#what-is-a-process-manager\">process managers</a> in the DDD community. Introducing a process manager into an architecture is a decision to model the process explicitly using the process manager rather than having the process modelled implicitly by having the aggregates communicate directly via commands and events. In the terminology of Chris Richardson you can therefore think of the process manager as an orchestrating saga.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h2 id=\"sagas-as-first-class-citizens\" tabindex=\"-1\">Sagas as first class citizens <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h2>\n<p>I was once working on a project where our aggregates were modelled using event sourcing and sagas were implemented as any other aggregate. Often an aggregate representing a business entity would also act as a saga, taking responsibility of calling other aggregates. The remote calls were carried out by event handlers on the domain events of the saga and  when the remote call finished a new domain event was written to the event store causing the next event handler to be triggered. There were a few problems with this approach.</p>\n<p>First of all an orchestrating saga is essentially a DAG consisting of the set of tasks that needs to be taken and ideally the code should reflect this. However in the approach depicted above the DAG was essentially modelled using reactive code, each task being represented by an event handler, and progressing to the next step was done by writing an event which then reactively triggered the next task. This impedance mismatch, the misalignment between what is being modelled and how it is represented in the code made it hard to read. It was quite difficult to get a holistic overview of the DAG because it required jumping around the code base between different events and event handlers. One symptom of this was that the team were maintaining UML diagrams of all the sagas that were implemented. These diagrams were necessary to keep up to date to help the developers understand the workflow, because it was simply too difficult (time consuming) to obtain a mental image of the workflow by reading the code itself.</p>\n<p>Secondly since the business entity and the saga was modelled by the same object it became difficult to separate orchestration problems from business problems. E.g. some remote calls were async which meant that it was necessary to write two events related to the call to the event store. First an event representing the intention to make the call and the following representing the result of the call. Obviously this meant that if we needed to make changes to the orchestration for technical reasons, replacing an async API with a sync API, implementing the change would involving making changes to the event stream which was also used to represent the state of the business entity.</p>\n<p>Another problem was how to handle event replay. Replaying events is a common practice in event sourcing which allows e.g. to rebuild views that are based on the events from the event store, however since the saga actions were implemented as event handlers on those events this could result in unwanted side effects as already completed sagas would then start making remote calls.</p>\n<p>Lastly implementing orchestrating sagas is non trivial. It involves keeping track of the saga state, implementing retries and keeping track of stuck or failed sagas and we discovered that operating the sagas involved many commonalities. A saga could get stuck due to some remote system not responding or an expected event not arriving, they could end up in an unexpected situation from which they couldn't progress needing human assistance. We found ourselves implementing the same  patterns over and over again, e.g. implementing observability and endpoints that allowed us to resume a stuck saga.</p>\n<p>In the end we decided to model sagas explicitly and keep all orchestration responsibilities out of the business entities. Since modelling the sagas were now independent of the domain logic it allowed us to implement a library providing generic saga functionality that could be reused across domains. In the following sections I'll go through some of this common functionality.</p>\n<h3 id=\"observability\" tabindex=\"-1\">Observability <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h3>\n<p>Keeping track of failed or stuck sagas is obviously quite important. The saga library maintained a database table listing all sagas, a row in the table showed the active task and a lastUpdated timestamp for a saga. Failed sagas were sagas that had ended up in the special <code>FailWithUnknownError</code> state and stuck sagas were sagas that had not made a state change in a while. The information in this table was used as the basis for prometheus metrics that reported on stuck and failed transfers and once alerted we could consult this table to get an overview of which sagas were having issues.</p>\n<p>The saga library was built using an in-house event sourcing library, that choice was not so much based on an opinion that modelling sagas using event sourcing was a good approach, but more on the fact that it was commonly used and we had a lot of experience with it. Using an event sourcing library meant that the entire saga history was automatically saved in the event store. This meant that in a debug scenario, apart from consulting the logs, if more details were needed regarding what data was sent or IDs used the saga event history could be consulted.</p>\n<h3 id=\"ops\" tabindex=\"-1\">Ops <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h3>\n<p>Many different types of orchestration related errors can happen in production. Some of them may cause a saga to become stuck or take wrong decisions and it must be possible to rectify these situations. E.g. a bug in the saga code, e.g. it was not able to recognize a valid response code from an API, could cause it to end up in <code>FailWithUnknownError</code>. In this case the corrective action would be to fix the bug in the saga code and move the saga back to the task that failed to execute. Of course the bug that caused the saga to misbehave could also originate from one of the remote systems that the saga interacts with. This could e.g. cause an API call to fail unexpectedly making the saga jump to the special <code>FailWithUnknownError</code> state. Again this must be fixed by fixing the bug in the remote system and move the saga back to the task that failed to execute.</p>\n<p>Much more complicated issues can arise. E.g. a bug can cause the saga to take wrong decisions and make incorrect remote calls, causing the distributed system to end up in an inconsistent state. In such situations it is often better to simply force complete the saga, and have a developer take over the responsibility for the state normally managed by the saga. If the APIs of the remote systems are available to be called manually the developer can make the corrective actions manually. The alternative, to enable the saga to get out of the situation by coding in the required flow will add complexity to the saga but since the situation was caused by a bug it is unlikely to ever end up in the same situation again so this part of the saga will not be useful later and only cause confusion to future developers who may not be familiar with the particular incident and understand why that particular behaviour is implemented.</p>\n<p>A <code>mission control</code> API could be used to manually control the saga behaviour at runtime. Here we list some of the available methods.</p>\n<ul>\n<li><strong>Next Task</strong>: Move the saga with the specified type and ID to the specified task.</li>\n<li><strong>Force Complete</strong>: Force completes the saga.</li>\n<li><strong>Retry Task</strong>: A saga is stuck in a task (it never moved to the next task, this can happen when using an <code>asyncTask</code>) and we'd like to manually retry it.</li>\n<li><strong>Update Data</strong>: A bug in the saga code caused it to write some wrong data to its state. This method can be used to mutate the saga data.</li>\n</ul>\n<p>The methods were exposed in our run script setup which has been described here <a href=\"https://example.com/dev-blog/blog/reducing_run/\">Running fast</a> This allowed us e.g. to quickly handle cases where many sagas had failed or gotten stuck due to the same reason, this was a simple matter of looping through the sagas in the saga overview table and calling the mission control function.</p>\n<h3 id=\"versioning\" tabindex=\"-1\">Versioning <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h3>\n<p>It is quite common to end up in a situation where it is necessary to make breaking changes to the saga flow. In this context breaking changes means a change to the saga flow that is incompatible with the existing flow, which means, if code changes were to be deployed while sagas using the old saga implementation were in-flight this would cause wrong behaviour. E.g.  changing the order of tasks or replacing one remote API with a another API could in some situations be breaking changes.</p>\n<p>We supported versioning by attaching a version property to a DAG and allowing to register multiple DAGs of different versions to the same saga. A saga was triggered by the <code>sagas.Start</code> method which takes in a version parameter. To make breaking changes to a saga the developer workflow was to create a new version of the DAG and register it on the saga, change the code that starts the saga to use the new version, wait for all sagas that were based on the deprecated DAG to complete in production and finally remove the old DAG from the code.</p>\n<p>If we had not built or used a saga framework then the saga implementations would have been entangled with the domain specific business logic. This means that figuring out how to make a breaking change to a saga flow would be a domain specific problem and would thus have to solved, in a new way, every time the problem arose.</p>\n<h3 id=\"postponement\" tabindex=\"-1\">Postponement <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h3>\n<p>Sometimes our sagas needed to wait for a period of time before continuing. This was also turned into a feature in the saga library that allowed a task to postpone itself for a specified period of time.</p>\n<h2 id=\"the-code\" tabindex=\"-1\">The code <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h2>\n<p>This post is not really a presentation of the specific library, but for completeness I'll show some examples.</p>\n<p>In the below example we're constructing a saga that will carry out some action (e.g. call some remote system) and if it fails it will carry out some compensating action. The example is a bit contrived, normally you wouldn't need to carry out a compensating action unless some previous actions actually succeeded, however to keep the example code short we skipped that part. The <code>done</code> task is a task provided by the saga library that will simply complete the saga.</p>\n<pre class=\"language-go\" tabindex=\"0\"><code class=\"language-go\">\n  <span class=\"token keyword\">type</span> SagaData <span class=\"token keyword\">struct</span> <span class=\"token punctuation\">{</span>\n    RememberThis <span class=\"token builtin\">string</span>\n  <span class=\"token punctuation\">}</span>\n\n  <span class=\"token keyword\">var</span> <span class=\"token punctuation\">(</span>\n    sagaName            <span class=\"token operator\">=</span> types<span class=\"token punctuation\">.</span><span class=\"token function\">Name</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"mysaga\"</span><span class=\"token punctuation\">)</span>\n    saga                <span class=\"token operator\">=</span> factory<span class=\"token punctuation\">.</span><span class=\"token function\">NewSaga</span><span class=\"token punctuation\">(</span>sagaName<span class=\"token punctuation\">)</span>\n    someAction          <span class=\"token operator\">=</span> tasks<span class=\"token punctuation\">.</span><span class=\"token function\">NewSomeAction</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    compensatingAction  <span class=\"token operator\">=</span> tasks<span class=\"token punctuation\">.</span><span class=\"token function\">NewCompensatingAction</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    done                <span class=\"token operator\">=</span> commontasks<span class=\"token punctuation\">.</span><span class=\"token function\">NewComplete</span><span class=\"token punctuation\">(</span>saga<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">)</span>\n\n  dag <span class=\"token operator\">:=</span> sagas<span class=\"token punctuation\">.</span>NewDag<span class=\"token punctuation\">[</span>SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>types<span class=\"token punctuation\">.</span>UnspecifiedVersion<span class=\"token punctuation\">)</span>\n  dag<span class=\"token punctuation\">.</span><span class=\"token function\">StartFrom</span><span class=\"token punctuation\">(</span>someAction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>\n    <span class=\"token function\">Transitions</span><span class=\"token punctuation\">(</span>\n      someAction<span class=\"token punctuation\">.</span>OnSuccess<span class=\"token punctuation\">.</span><span class=\"token function\">GoTo</span><span class=\"token punctuation\">(</span>done<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      someAction<span class=\"token punctuation\">.</span>OnFailure<span class=\"token punctuation\">.</span><span class=\"token function\">GoTo</span><span class=\"token punctuation\">(</span>compensatingAction<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      compensatingAction<span class=\"token punctuation\">.</span>OnSuccess<span class=\"token punctuation\">.</span><span class=\"token function\">GoTo</span><span class=\"token punctuation\">(</span>done<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span>\n\n  saga<span class=\"token punctuation\">.</span><span class=\"token function\">RegisterDag</span><span class=\"token punctuation\">(</span>dag<span class=\"token punctuation\">)</span></code></pre>\n<p>Notice how the DAG is explicitly expressed in the code. Also notice that each task is implemented as a separate type. We keep each task in a separate go file and this means that it is easy to get an overview of which tasks exist when looking at the code in the tree navigator. It becomes immediately apparent to the reader that there is a concept called a saga and a task when looking at the folder structure, which was certainly not the case previously where the saga and tasks were buried within events and event handlers. This is an example of applying the principle of <a href=\"https://blog.cleancoder.com/uncle-bob/2011/09/30/Screaming-Architecture.html\">screaming architectures</a></p>\n<p>Also notice how the saga and DAG construction is separated. A DAG is constructed and registered on the saga. This will make it possible to handle versioning by registering multiple DAGs on the same saga.</p>\n<p>Next let's take a look at how the tasks are implemented. The <code>someAction</code> task declares two connectors, OnSuccess and OnFailure. In the implementation of Execute it will, based on the outcome of the side effect that is executed, decide where to go next, to the OnSuccess or to the OnFailure task. The connectors has two type arguments, one being the SagaData type which must correspond the SagaData type on the saga and an Args type which is the type of arguments that the task can take. The connectors allow us to reuse a saga task implementation in multiple sagas by simply plugging in different tasks in the connectors when constructing the DAGs.</p>\n<pre class=\"language-go\" tabindex=\"0\"><code class=\"language-go\"><span class=\"token keyword\">type</span> someAction <span class=\"token keyword\">struct</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token operator\">*</span>sagas<span class=\"token punctuation\">.</span>Task<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span>\n\n  OnSuccess <span class=\"token operator\">*</span>sagas<span class=\"token punctuation\">.</span>Connector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span>\n  OnFailure <span class=\"token operator\">*</span>sagas<span class=\"token punctuation\">.</span>Connector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">func</span> <span class=\"token function\">NewSomeAction</span><span class=\"token punctuation\">(</span>\n<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span>someAction <span class=\"token punctuation\">{</span>\n  task <span class=\"token operator\">:=</span> <span class=\"token operator\">&amp;</span>someAction<span class=\"token punctuation\">{</span>\n    Task<span class=\"token punctuation\">:</span> sagas<span class=\"token punctuation\">.</span>NewTask<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">}</span>\n  task<span class=\"token punctuation\">.</span>OnSuccess <span class=\"token operator\">=</span> sagas<span class=\"token punctuation\">.</span>NewConnector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">)</span>\n  task<span class=\"token punctuation\">.</span>OnFailure <span class=\"token operator\">=</span> sagas<span class=\"token punctuation\">.</span>NewConnector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">return</span> task\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">func</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">*</span>someAction<span class=\"token punctuation\">)</span> <span class=\"token function\">Execute</span><span class=\"token punctuation\">(</span>ctx context<span class=\"token punctuation\">.</span>Context<span class=\"token punctuation\">)</span> sagas<span class=\"token punctuation\">.</span>Decider<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">return</span> <span class=\"token keyword\">func</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">_</span> sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> state <span class=\"token operator\">*</span>saga<span class=\"token punctuation\">.</span>State<span class=\"token punctuation\">)</span> saga<span class=\"token punctuation\">.</span>Decision<span class=\"token punctuation\">[</span>SagaData<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">if</span> rand<span class=\"token punctuation\">.</span><span class=\"token function\">Intn</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">50</span> <span class=\"token punctuation\">{</span>\n      <span class=\"token comment\">// well that didn't work, we must follow the rainy day path</span>\n      <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span><span class=\"token function\">Next</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>OnFailure<span class=\"token punctuation\">.</span><span class=\"token function\">Resolve</span><span class=\"token punctuation\">(</span>state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">)</span> \n    <span class=\"token punctuation\">}</span>\n    <span class=\"token comment\">// all good, follow the happy path</span>\n    <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span><span class=\"token function\">Next</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>OnSuccess<span class=\"token punctuation\">.</span><span class=\"token function\">Resolve</span><span class=\"token punctuation\">(</span>state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre>\n<p>The <code>compensatingAction</code> task has a single connector, OnSuccess, there is no OnFailure connector as there is no expected way it can fail. If it fails for unexpected reasons the task will move the saga into the <code>FailedWithUnknownError</code> state. This is a special state that all sagas can end up in. When a saga is in that state it must be moved out of the state manually by calling one of the methods on the <code>mission control</code> API that we will discuss in detail in a later section.</p>\n<p>The <code>FailedWithUnknownError</code> state is used to handle all the situations that cannot be handled automatically, either because they were unexpected and therefore the saga code did not take that scenario into consideration, or because there simply is no supported way to handle the situation automatically. We will discuss this in more detail later.</p>\n<pre class=\"language-go\" tabindex=\"0\"><code class=\"language-go\"><span class=\"token keyword\">type</span> compensatingAction <span class=\"token keyword\">struct</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token operator\">*</span>sagas<span class=\"token punctuation\">.</span>Task<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span>\n\n  OnSuccess <span class=\"token operator\">*</span>sagas<span class=\"token punctuation\">.</span>Connector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">func</span> <span class=\"token function\">NewCompensatingAction</span><span class=\"token punctuation\">(</span>\n<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span>compensatingAction <span class=\"token punctuation\">{</span>\n  task <span class=\"token operator\">:=</span> <span class=\"token operator\">&amp;</span>compensatingAction<span class=\"token punctuation\">{</span>\n    Task<span class=\"token punctuation\">:</span> sagas<span class=\"token punctuation\">.</span>NewTask<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">}</span>\n  task<span class=\"token punctuation\">.</span>OnSuccess <span class=\"token operator\">=</span> sagas<span class=\"token punctuation\">.</span>NewConnector<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">return</span> task\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">func</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">*</span>compensatingAction<span class=\"token punctuation\">)</span> <span class=\"token function\">Execute</span><span class=\"token punctuation\">(</span>ctx context<span class=\"token punctuation\">.</span>Context<span class=\"token punctuation\">)</span> sagas<span class=\"token punctuation\">.</span>Decider<span class=\"token punctuation\">[</span>sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> SagaData<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">return</span> <span class=\"token keyword\">func</span><span class=\"token punctuation\">(</span><span class=\"token boolean\">_</span> sagas<span class=\"token punctuation\">.</span>EmptyArgs<span class=\"token punctuation\">,</span> state <span class=\"token operator\">*</span>saga<span class=\"token punctuation\">.</span>State<span class=\"token punctuation\">)</span> saga<span class=\"token punctuation\">.</span>Decision<span class=\"token punctuation\">[</span>SagaData<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">{</span>\n    err <span class=\"token operator\">:=</span> <span class=\"token function\">makeRemoteCall</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> err <span class=\"token operator\">!=</span> <span class=\"token boolean\">nil</span> <span class=\"token punctuation\">{</span>\n      <span class=\"token keyword\">if</span> errors<span class=\"token punctuation\">.</span><span class=\"token function\">Is</span><span class=\"token punctuation\">(</span>err<span class=\"token punctuation\">,</span> BadRequest<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token comment\">// something really unexpected happened, I have no idea what to do now!</span>\n        <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span>FailWithUnknownError<span class=\"token punctuation\">[</span>SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>err<span class=\"token punctuation\">.</span><span class=\"token function\">Error</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token punctuation\">}</span>\n      <span class=\"token comment\">// this error is probably retriable</span>\n      <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span>Retry<span class=\"token punctuation\">[</span>SagaData<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>err<span class=\"token punctuation\">.</span><span class=\"token function\">Error</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">}</span>    \n\n    <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span><span class=\"token function\">Next</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>OnSuccess<span class=\"token punctuation\">.</span><span class=\"token function\">Resolve</span><span class=\"token punctuation\">(</span>state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre>\n<p>Notice that if for some reason the saga framework fails to persist the decision about where to go next this will also result in the call to Execute to be automatically retried. Therefore it is essential that the side effect carried out in the Execute method is idempotent.</p>\n<p>A task can mutate the saga data. In this example it could set the value of the <code>RememberThis</code> property and that value would be available to subsequent tasks. It is also possible to pass on arguments to tasks in the sagas.Next method. In this example the tasks did not declare any arguments so the special value <code>args.Empty</code> was used.</p>\n<h2 id=\"conclusion\" tabindex=\"-1\">Conclusion <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/sagas/\">#</a></h2>\n<p>As this post hopefully demonstrates orchestration is a non trivial problem and many of the challenges that arise are of a generic nature. Moving away from a domain specific approach to using a generic library allowed us to save a lot of time and if you have to deal with a lot of orchestration it is something that I can definitely recommend.</p>\n",
			"date_published": "2024-09-27T00:00:00Z"
		}
		,
		{
			"id": "https://example.com/dev-blog/blog/data_platform/",
			"url": "https://example.com/dev-blog/blog/data_platform/",
			"title": "How to reach production in less than 6 months",
			"content_html": "<p>Some projects will have deadlines imposed by the business. They could originate from time-to-market considerations or from specific contract obligations, and for those projects delivery time is obviously important. However, I would ague that even on projects that have no hard deadlines, the time to reach production is still vital. Reaching production sooner rather than later builds trust with stakeholders and the developers will get a feeling that they're making an impact. It also closes the feedback loop. Getting feedback from experiments or test users from production is often the most valuable type of feedback and will help the agility of the project going forward.</p>\n<p>In this post I share my experiences related to reducing the time to reach production on greenfield projects. In particular I will be referring to my experiences with building a data platform from scratch. It was a big undertaking, I was a one man army, and it was therefore absolutely vital to be pragmatic and diligent about prioritisation to ensure that the product could provide value to the business in a reasonable amount of time.</p>\n<h2 id=\"what-is-the-mvp\" tabindex=\"-1\">What is the MVP <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/data_platform/\">#</a></h2>\n<p>Obviously you'll need to define the minimal viable project. What is the smallest set of requirements that you can deliver while still having a viable value proposition? If your product is replacing an existing product you must find out what capabilities that product has and you must take care to deliver a better alternative, or at least not something that is significantly worse. If the existing product is comprehensive you could consider replacing only parts of it, and then carry out the sunsetting in iterations. This will of course need to be a collaboration between the development team and stakeholders and/or product owners. If you're building a platform product it could be feasible to set up regular feedback sessions with your target users to help prioritise and shape the MVP.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>The company I was working with already had an existing product. The product consisted of a postgreql database, a BI tool, and a microservice that was subscribing to event data coming from other services from the backend and mapping the data into fact and dimension tables in the postgresql database.</p>\n<p>The company was a startup and the initial motivation behind building the existing product was to make it possible for people from the business to keep track of various KPIs via the BI tool. The set of KPIs were fairly static so the solution worked ok for a while, but it didn't take long before other analytical use cases appeared. Those new use cases came with the need to get more data into the database and to execute queries that did not fit the facts and dimensions structure that had been built and did not utilize the indexes. Making these sorts of changes was done in the microservice. It was responsible for ingesting, cleaning and mapping the source data to the table structure in the database, and so to add a new data source a go developer was needed to add a subscription to the new event and create the needed mappings. This was painful, the go developer was not personally invested in the analytical use cases and this felt like a chore to him, and the analyst was stuck waiting for the go developer to prepare the tables. To make matters worse, most often it was impossible to backfill data, so when a new subscription to an event was created it was not possible to get hold of all the historical data that had been published before the subscription was created. This meant in practice that for data to be available for an analyst it would be required that he was able to predict which types of analytical queries he would be interested in making at the time when a new event started to be published from the backend. New analytical use cases pop up all the time so this is clearly not feasible.</p>\n<p>What the company needed was a data warehouse that supported OLAP queries without requiring up front indexing. Such a data warehouse could obviously also easily solve the existing use case of making KPIs available to the business. Alleviating the pain associated with adding new data sources and supporting new queries and also solving the performance issues they were facing due to the postgresql database struggling to keep up with the large amounts of data was a very appealing value proposition and something &quot;worth waiting for&quot;. However, the existing product had not taken years to build, so it would not be reasonable to spend years building a data platform without delivering a product that could provide some value.</p>\n<p>The absolute minimal set of requirements were determined to be:</p>\n<ol>\n<li>A data warehouse.</li>\n<li>An ingestion pipeline that ingests the data from the backend into the data warehouse and makes it available for analysts.</li>\n<li>Tooling that will make it possible to refine the raw event data in the data warehouse into better suited table structures. The tooling must be SQL based so analysts can own this.</li>\n</ol>\n<p>The initial product did not need to make ALL data from the backend available in the data warehouse, only the data that was currently in use was necessary. The SQL tooling did not need to be perfect, it just needed to be less painful than the existing solution, which was a very low bar.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h2 id=\"compromise\" tabindex=\"-1\">Compromise <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/data_platform/\">#</a></h2>\n<p>You will need to cut corners. Identify what are need-to-haves and what you can be less diligent about. You probably want to ensure durability and avoid loosing data, but perhaps the UX/DX doesn't need to be perfect?</p>\n<p>You'll need to take calculated risks. E.g. if you release a component without testing it, what would be the impact to the users if a bug creeped into production, how would you resolve the issue and how long would it take to resolve it? It can be ok to fail in production if the impact is small and the time to recover is low, but if the production issues result in a large maintenance burden the ROI of implementing it and testing it properly before releasing it could be above 0.</p>\n<p>The compromises you make will result in feature-incompleteness or perhaps a less reliable product. If you keep track of user/developer pains you'll be aware of these shortcomings and eventually they will get prioritised. However, you will feel a sting as it will conflict with your professional pride.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>We chose a tool called DBT for providing SQL based transformation capabilities to the users of the data platform. DBT was in the early stages at the time (2019), there was no dbt cloud and so we had to roll our own execution environment. We did this by copying the user code into a docker image containing a dbt installation as part of our build pipeline, and then we would run this image on kubernetes (using airflow as a scheduler). The docker image was very simple, it contained an entrypoint.sh shell script that included a <code>dbt run</code> command.</p>\n<p>As time passed new requirements meant that the simple <code>dbt run</code> command needed to be extended with more options and we did this by extending the shell script. There were no tests of the shell script and sometimes bugs would sneak into the script causing the execution of dbt to fail in airflow. This was obviously an inconvenience to users who more often than not had no clue what was causing the error to occur. Most often the error was detected and corrected in a few minutes but it was still causing developer pains and a great source of instability, yet it was allowed to stay this way for 6 months.</p>\n<p>Eventually we finally got around to making this more robust. The shell script was converted to python code and acceptance tests were added.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h2 id=\"ops\" tabindex=\"-1\">Ops <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/data_platform/\">#</a></h2>\n<p>Going to production as early as possible entails having to operate immature software. This will most likely result in many production issues and a lot of related &quot;run&quot; work, and you'll need to have a plan for how to handle this. If you end up spending all your time doing run work the project will eventually stall.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>One of the classical problems faced by data platforms is how to get hold of the schemas of the data that is being ingested. The data that was ingested was based on rabbitmq messages exchanged between the backend services written in go and the schema definitions were written in go code which was buried inside of a larger shared go project and since the data platform was not written in go, it was written in scala and python, the schemas were not directly accessible.</p>\n<p>In a situation where schema definitions are not readily available getting hold of the schema definitions will require manual effort, and one must decide where this responsibility lies. Does the responsibility lie with the data producers or with the consumers, in this case the data platform? Generally, this responsibility should lie with the data producers because if producers are required to provide a valid schema it allows anyone to consume the data without hassle, but in this case all consumers except for the data platform were go microservices and were therefore able to use the schemas provided in the go project. Moving to a language agnostic schema format, would require a great deal of discussions, alignments and agreements across the entire organisation. At the time, there was no formal RFC process in place and the data platform team had no mandate to make such requirements. Changing the schema definition process would require a lot of work for all backend teams and there was simply no appetite to prioritise this as the pain was only felt by the data teams. For this reason the responsibility of getting hold of the schemas fell on the data platform.</p>\n<p>One popular solution in these situations is to extract schemas using schema inference. With schema inference the schemas in the platform are automatically inferred and dynamically adapted to the data as it is being ingested. Schema inference causes a lot of headaches, especially if many heterogenous (polyglot) sources are at play. E.g. Is “7&quot; an integer or a decimal number? (some languages/frameworks leaves out the decimal points when it is 0). Is “.” a decimal point or a thousands separator? These imperfections of schema inference inevitably results in inaccurate schemas and eventually in data that cannot be ingested because of mismatching schemas. Another common problem is that schemas can evolve in a non compatible way causing errors to occur when the schema inference process tries to register the new version of the schema.</p>\n<p>Instead we chose to build tooling that could extract avro schemas from the aforementioned go project. The schemas in the go code were not defined in a declarative way, i.e. it was not as easy as scanning for structs and using the fully qualified name of the struct as the schema name. It was, however, possible to parse the go code of the project, traversed the AST and then by making a number of assumptions on the structure of the code extract the information that was needed from the code and generate the schemas in avro format. Luckily, it turned out that there was a lot of conformity among the developers who all adhered to the existing code structure when extending the code, which meant that the solution wasn’t as brittle as one might have feared. Nevertheless, because of the nature of how we had to extract the schemas and because we wanted to allow developer teams to experiment in dev/prod which might entail introducing breaking changes to schemas, there was a high probability that issues related to faulty schemas could cause ingestion of messages to halt and create noise in the logs that would require the attention of the engineers responsible for the platform and in many cases this work would be <code>urgent + not important</code> because the errors would be related to data that was not in use by the data analysts. We therefore chose not to automate the schema registration process but instead create a CLI that allowed a data analyst to generate and register a schema on demand using the tooling described above.</p>\n<p>Because we needed to capture all the messages that were exchanged between the backend services and we required schemas to be defined upfront we needed to allow for schemaless data to exist in the data platform. To this end we decided to create two data lakes. An unstructured data lake and a structured data lake. The purpose of the unstructured data lake was to capture everything. It would contain all incoming data in its raw format (json in this case) without requiring a schema to be present. The structured data lake would contain the subset of data that had a schema defined.</p>\n<p>Any issues related to schemas, e.g. issues with generating schemas from the go code, or schemas not being compatible with the ingested data, would now either occur in the tooling or in the data pipeline that applied the schema from the raw data. It would not effect the data pipeline that ingested the raw data into the unstructured data lake. I.e. the most critical requirement, not loosing any data, would be uncoupled from the inherent unstable process of applying schemas to the data, and thus those errors which we were expecting to see a lot of, especially in the beginning, were now no longer urgent and critical errors. Furthermore if schema related issues arose, users of the data platform could fix the invalid schema and run a backfill operation to correct the wrong data in the structured data lake, and all of this could be done without the presence of a member of the data platform team. Building two data lakes and two data pipelines rather than one is a larger effort, but the expectation was that the amount of avoided run work would quickly make up for this investment. (There are also many other advantages to making this split in the data lake that we will not get into in this post.)</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<p>See <a href=\"https://example.com/dev-blog/blog/fail_fast/\">Ops driven design</a> for a more general discussion on this topic.</p>\n<h2 id=\"single-responsibility\" tabindex=\"-1\">Single responsibility <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/data_platform/\">#</a></h2>\n<p>As mentioned, it might make sense to consider cutting corners in order to reach production in a reasonable amount of time. Cutting corners will have consequences, such as less stability or usability of the product, which can be a reasonable compromise to make in certain situations. I would, however, advise against violating the single responsibility principle. It can be very tempting to cram together multiple concerns into a single service, tool or CLI. It is, after all, faster to build a single service and a single API rather than multiple ones and there will be less movable parts.</p>\n<p>However, you should consider the consequences of doing so. If the idea is to move fast and reach production in the shortest possible time, then when you reach production you'll have an unfinished product that you will need to develop further. When you violate the single responsibility principle it will be more difficult to evolve the product because different concerns, that you'll most likely want to evolve independently, are entangled in the architecture. This will hinder further development as you'll often find you'll need to refactor a component before you can evolve it. Reaching production fast only to slow down significantly later is not a good strategy.</p>\n<p>Instead try to understand your problem space and build up your solution based on components that each serve a specific purpose. That way, when the product needs to be evolved, the individual components can be replaced or extended independently.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>As part of onboarding a new user on the data platform we needed to set up her development environment which included a personal database on a redshift  cluster. This personal database was used to run dbt locally as part of the development work flow (when developing new data models using dbt).\nSetting this up involved executing some DDL statements on the redshift cluster for setting up the database, the user and permissions, and it would be necessary to repeat this if we wanted to provision a new redshift cluster, e.g. to increase capacity. We therefore wanted to automate this.</p>\n<p>To comply with EU GDPR regulations it was important to limit access to PII (personally identifiable information). To this end we wanted to implement an RBAC mechanism, redshift did not support this at the time, where users would need to take on purpose specific roles in order to gain access to tables containing PII. To allow users to manage roles and permissions for those roles we needed a service that could take such a configuration and execute the required DDL statements to set up the users and permissions on the redshift database. This was a very similar problem to the problem described above of setting up personal databases.</p>\n<p>We decided to implement a single service to handle both use cases. Users would maintain a single configuration file (protected by the 4 eyed principle) listing all users, all roles, and which users had access to which roles, and a backend service (a kubernetes controller to be exact) would apply the changes to the redshift clusters.</p>\n<p>Now let's say we wanted to migrate to dbt cloud. All the logic related to setting up the development environment would need to be removed from the service, one would need to take care not to inadvertently change the rest of the logic. Similarly, let's say redshift implemented support for RBAC, we would no longer be required to maintain our own solution. One would need to remove that part from the service and again take great care not to touch the code that set up the development environment. Had each use case been implemented by separate services, or at least clearly separated in the service, evolving the product would have been easier.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n",
			"date_published": "2024-08-02T00:00:00Z"
		}
		,
		{
			"id": "https://example.com/dev-blog/blog/reducing_run/",
			"url": "https://example.com/dev-blog/blog/reducing_run/",
			"title": "How to run faster",
			"content_html": "<p>Some systems are of a complexity that a certain baseline of <code>run work</code> is unavoidable. If the system is large or complex or if there's a lot of ongoing feature development, issues will arise. In addition, sometimes systems are plagued with legacy and getting out of that situation is not something that can be achieved in the short to mid term.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Change - Improve - Run</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>It is common practice to split the work of a software engineer into three types: <code>change</code>, <code>run</code> and <code>improve</code>.\n<code>Change</code> denotes feature development, <code>run</code> is work needed to keep the system running and <code>improve</code> work are tasks that reduce the amount of run. Fixing a bug that is causing issues in production is an example of improve work because it will remove the run work associated with handling the incidents. Project managers will often pay attention to the proportion of run work that is required by a team and prioritize improve work accordingly. If run work is taking up a large amount of time it could be an indicator that improve work is not being sufficiently prioritized.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<p>If you find yourself in a situation where run work is taking up a significant chunk of your time and there is nothing you can do about it from a project management perspective you should look at how you can optimize your run workflow. If you spend a lot of your time digging through logs, logging into various systems, copy pasting strings between applications etc. it might be worth considering how you can reduce this overhead. In this post I give an example of what I did on a project to help myself solve the run tasks faster.</p>\n<p>DISCLAIMER: The code samples shown in this post are for illustration purposes and are not extracted from any real system.</p>\n<h2 id=\"context\" tabindex=\"-1\">Context <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>The backend was built using a microservice architecture, the services were running in kubernetes. Databases were running in AWS RDS. It was possible to get access to the database of a service via psql in the terminal. If an incident had caused bad state to be written to the database it was common practice to implement so called <code>mission control</code> endpoints for the service that could be used to execute a corrective action. It was also possible to request write access to the database and correct the data directly via sql. The mission control endpoints were usually implemented using graphql and engineers would execute their mission control endpoints via the graphql playground UI directly in the browser.</p>\n<p>There were several annoyances with this setup:</p>\n<ul>\n<li>It was necessary to have a terminal open for each service database that an engineer would need to access, or alternatively log out and log back in via psql, and data queried from one service database could not be joined with data queried from another service database.</li>\n<li>Executing the mission control endpoints were done in the browser, so all parameters would need to be passed into the playground UI. Sometimes an incident had caused issues related to many entities so that could entail constructing long list of entity IDs that would need to be pasted in. This was time consuming and error prone.</li>\n<li>The centralised log were accessed via the browser. It was common to use the logs to obtain IDs of entities that would need to be inspected in the database. The IDs from the logs would need to be copied and pasted into psql as valid SQL, also a time consuming and error prone task.</li>\n</ul>\n<h2 id=\"scripting\" tabindex=\"-1\">Scripting <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>We wanted to find a way to replace the manual process with a scripting approach. Scripting run work has the obvious benefit that the code becomes an artifact and can be submitted to a git repository like all other code.</p>\n<ul>\n<li>A run script can be reviewed before being executed. This can be useful depending on the risk factor of the specific run script.</li>\n<li>Previous run work serves as documentation. It can be used as inspiration on how to solve a run task.</li>\n<li>Previous run work is automatically documented. If a database update carried out by a mission control endpoint is later observed by an engineer it can be traced back to a run script which can provide context.</li>\n</ul>\n<h2 id=\"pycharm\" tabindex=\"-1\">Pycharm <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>We were looking for a unified environment where an engineer could log in once and gain access to all service databases and mission control endpoints and where data could be joined across databases and systems. Python is one of the most popular scripting languages and it turned out that the PyCharm IDE had everything we needed. We created a simple folder structure, one folder per week, and when an engineer was solving a run task he would put the corresponding script into the folder of the week. Quite often a run task would be similar to other run tasks and any reusable code was put into another package that could be referenced from all the run scripts.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/riyQh31oeh-458.avif 458w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/riyQh31oeh-458.webp 458w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/riyQh31oeh-458.png\" width=\"458\" height=\"390\"></picture></p>\n<p>Pycharm provides a REPL (called python console) and Jupyter notebooks out of the box. Both are very suitable when solving run tasks and we will get into more details about this later.</p>\n<h2 id=\"everything-is-a-dataframe\" tabindex=\"-1\">Everything is a dataframe <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>To ensure data from multiple data sources could be joined and correlated and to ensure a uniform experience we adopted an <code>everything is a dataframe</code> principle in the code base, i.e. all functions that return data should return the data in the form of a <a href=\"https://pandas.pydata.org/\">pandas</a> dataframe. This way the capabilities of pandas can be used to join, group, aggregate, plot and export data.</p>\n<h2 id=\"dry-run\" tabindex=\"-1\">Dry run <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>A global <code>dry_run</code> boolean variable is defined that defaults to <code>True</code>. When enabled all functions that perform side effects should run in <code>dry run</code> mode, i.e. not perform the side effects but instead print out the intended action. This will allow engineers to test their scripts and the associated side effects before executing it. It will also make it possible to create a PR containing the test script including its output that can be reviewed before it is executed. We will get into more details about how this works in the <a href=\"https://example.com/dev-blog/blog/reducing_run/\">Notebooks</a> section.</p>\n<h2 id=\"console\" tabindex=\"-1\">Console <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>Pycharm provides a REPL (called python console) out of the box. The interactive nature of the console makes it perfect for exploration and debugging scenarios. E.g. sometimes you'll find that your logging verbosity is insufficient and you'll need to look at the actual state of your application to understand what has happened. We used the console as a way to gain access to all the databases of our services. In this section we'll go through an example scenario to illustrate the usefulness of the console.</p>\n<p>Imagine you're looking at a dashboard and is discovering that some sagas have gotten stuck:</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.avif 900w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.webp 900w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.png\" width=\"900\" height=\"294\"></picture></p>\n<p>The next step would be going to the python console in Pycharm and trying to find out what happened. When the console is opened the user will get logged in and a prompt appears.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/Q1W0IgFStN-900.avif 900w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/Q1W0IgFStN-900.webp 900w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/Q1W0IgFStN-900.png\" width=\"900\" height=\"337\"></picture></p>\n<p>After looking up the failed sagas in the console it turns out that a few of them failed due to a null pointer exception. Turns out there was a bug in the code triggered by an unexpected edge case. We fix the bug in the code and redeploy. The sagas that failed with this particular error can now be retried.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/Kd1ULALOp4-1000.avif 1000w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/Kd1ULALOp4-1000.webp 1000w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/Kd1ULALOp4-1000.png\" width=\"1000\" height=\"622\"></picture></p>\n<p>A <code>sagas.py</code> file, that was shared across all services, declared various functions that could be used to manage sagas. The reason the code in <code>sagas.py</code> could be shared across services was that the services were using the same saga library. The function for retrieving the set of failed sagas is very simple. It simply fetches the data from a database table and loads it into a pandas dataframe. Here's an excerpt from the <code>sagas.py</code> file:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> utils <span class=\"token keyword\">import</span> db\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_failed</span><span class=\"token punctuation\">(</span>db_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    query <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"\"\"select * from supervisor_view where completed=false and error_message is not null\"\"\"</span></span>\n    result <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_sql<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">,</span> db<span class=\"token punctuation\">.</span>sqlalchemy_connect<span class=\"token punctuation\">(</span>db_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> result</code></pre>\n<p>The <code>transfer_service.py</code> file called this function:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">db_name <span class=\"token operator\">=</span> <span class=\"token string\">'transferservice'</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_failed_sagas</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> sagas<span class=\"token punctuation\">.</span>get_failed<span class=\"token punctuation\">(</span>db_name<span class=\"token punctuation\">)</span></code></pre>\n<p>Let's say you needed more domain insights. As an example, it could be necessary to find out the amount of funds that had been withheld due to the stuck transfers. This can be achieved by looking up details from the transfer aggregates.</p>\n<p><em>In this case the ID of the transfer aggregates are the same as the corresponding sagaID (there is one saga for each aggregate), and to get the state of an aggregate a function called <code>state_apply</code> can be called which will return one row for each aggregate. The name state_apply comes from the fact that we were using event sourcing to store the state of the aggregates so getting the state of an aggregate implies folding the events of the aggregate event stream. We'll not go into more detail about this here.</em></p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/tWSGXUGnS7-900.avif 900w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/tWSGXUGnS7-900.webp 900w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/tWSGXUGnS7-900.png\" width=\"900\" height=\"310\"></picture></p>\n<p>Similary data can be fetched from other data sources, e.g. application logs, and because of the <code>everything is a dataframe</code> principle the data can be joined, grouped, aggregated and plotted freely.</p>\n<h2 id=\"notebooks\" tabindex=\"-1\">Notebooks <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>Pycharm will open the notebook view for files of type <code>ipynb</code> and will automatically start a local jupyter notebook server for you. Notebooks has a sequence of cells that contain the python code. If the python code in a cell returns anything or prints to standard out it will be printed in the notebook after the cell. Notebooks are very useful when the run script is complex or requires some diagnosis/analysis before the fix can be determined and applied, because the notebook &quot;tells a story&quot;.</p>\n<p>In the example given in the notebook below a saga got stuck because something unexpected happened, i.e. a service returned an error that the saga could not handle. In this case the easiest way to solve the issue was to do a manual corrective action, i.e. transfer some funds, and then force complete the saga. I.e. instead of trying to extend the saga implementation to be able to handle the pathological case the engineer takes over the responsibility of the specific flow from the saga by doing a manual action and then completing the saga.</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/aU855m05ui-1000.avif 1000w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/aU855m05ui-1000.webp 1000w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/aU855m05ui-1000.png\" width=\"1000\" height=\"676\"></picture>\n<picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/xVBOVdX5LE-1000.avif 1000w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/xVBOVdX5LE-1000.webp 1000w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/xVBOVdX5LE-1000.png\" width=\"1000\" height=\"578\"></picture></p>\n<p>Since this case involves transferring of funds it might be a good idea to have the script reviewed by a peer before executing it. That can be done by executing the notebook and answering yes in the input dialog that appears when the <code>dry_run.ask()</code> statement is executed. That way all cells will run and intended side effects will get printed as output but no side effects are executed. A PR containing the notebook output can then be reviewed by a peer before the notebook is finally executed with dry run disabled.</p>\n<h2 id=\"integrations\" tabindex=\"-1\">Integrations <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>The backend APIs all required valid JWTs. Obtaining a valid jwt for the active user was fairly simple since the company was using okta. We defined a function <code>get_access_token</code> which would initiate a device authorization flow and fetch a new jwt token via the okta api, unless a valid token was already found locally. This function was used anywhere we needed a jwt token:</p>\n<pre class=\"language-python\" tabindex=\"0\"><code class=\"language-python\">  headers<span class=\"token punctuation\">[</span><span class=\"token string\">'authorization'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"Bearer </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>get_access_token<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span></code></pre>\n<p>Communication with the graphql APIs was done by forwarding to the remote port on kubernetes using the python kubernetes sdk. A configuration file <code>services.json</code> contained the names of all the backend services and a small script <code>generate_graphql.sh</code> could be used to generate client code for the graphql APIs of all the services.</p>\n<p>Communication with the service databases was also achieved using port forwarding. Credentials were obtained via the AWS RDS python client. If the aws credentials of the user had expired the python code would automatically initiate a login flow for the user in the browser by running <code>aws sso login</code>.</p>\n<h2 id=\"caveats\" tabindex=\"-1\">Caveats <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/reducing_run/\">#</a></h2>\n<p>The solution depicted in this post, the interactive console and the notebooks, works so well because the language supports it. Most dynamically typed languages are well suited but even some statically typed languages, e.g. Scala, has similar features. However, your backend team might be using a language where this sort of tooling is difficult to build. In our case the language in use was not python so building and using the tooling required team members to learn python at a basic level, including parts of the standard library (date time and string manipulation etc.). Understanding and using pandas dataframes is also not trivial for engineers with no experience in the python language or data analysis.</p>\n<p>Another point is that interacting directly with database tables obviously breaks encapsulation, meaning that the scripts are likely to be quite brittle. This is not a huge concern as the scripts are part of run work, i.e. by definition a temporary solution to a temporary problem, so the code is expected to have a short timespan after which it is no longer needed. In addition, if the code breaks it is of less criticality as it is run manually by an engineer. Still it can be a challenge to keep the code working over time, and when a critical problem arises it can be very inconvenient if code needs to be refactored before a fix can be applied.</p>\n",
			"date_published": "2024-07-24T00:00:00Z"
		}
		,
		{
			"id": "https://example.com/dev-blog/blog/fail_fast/",
			"url": "https://example.com/dev-blog/blog/fail_fast/",
			"title": "Reduce time to recover and embrace failing",
			"content_html": "<p>When your greenfield project finally hits production reality will bite you. Bugs will surface, edge cases you never thought about will emerge, the system will get into a bad state and you'll need to diagnose and correct the issues when they arise. This work, often referred to as &quot;run&quot; work, will be important and urgent and it will take up a lot of your time.</p>\n<p>Did you build your solution in a way that makes it easy to operate? If you didn't, this kind of work can take up much more of your time than it should, time that you could otherwise have spent on building new features and (hopefully) make an impact.</p>\n<h2 id=\"functional-requirements\" tabindex=\"-1\">Functional requirements <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/fail_fast/\">#</a></h2>\n<p>Are you keeping a critical eye on the functional requirements? Are they reasonable and suitable for a first release of the product? Or are they just making the solution more difficult without much or any benefit?</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>One of the initial requirements I was presented with during the early days of implementing a data platform was the need for having BI dashboards updated in real-time. It was important for the people to know the KPIs of the business and the fresher the data the better!</p>\n<p>At the time implementing support for real-time data transformations was much more challenging than supporting batch. In batch mode you could simply run the SQL transformations directly on the data warehouse at regular intervals, in real-time mode you would need to leverage technologies such as flink or spark streaming. The transformations were built by the users of the platform and could be of varying quality resulting in a heavy ops burden when the those transformations went rogue.</p>\n<p>After taking a critical look at the requirement for real-time updated dashboards it turned out that there were no real use cases requiring it. Data updated at hour intervals was good enough for all known use cases. Having dashboards updated in real-time was &quot;cool&quot; but at the end of the day there was no real need. Cutting away this requirement allowed for a much simpler initial solution and therefore also less potential run work.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<p>Perhaps there are some small alterations you can make to the functional requirements that will have little impact on the product but a huge impact on the complexity of the solution? A good example of this is the one given by Martin Kleppmann in &quot;Designing Data-Intensive Applications&quot;. In the example a booking system has relaxed the requirement of strong consistency and instead opted for eventual consistency. This means that two users could be booking the same tickets at the same time without knowing. The proposal is to allow for this to happen and then notify the users over email afterwards that their booking didn't go through after all. Depending on how likely such a scenario is to happen and how much negative impact it has on the users it can be a reasonable choice to make. Guaranteeing strong consistency in a distributed system is a hard problem after all.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>Once I was working in a company where the primary product was a mobile app. In one scenario, a user would input an &quot;order&quot; in the app which would trigger an orchestration process on the backend. While the backend process was running the user was presented with a &quot;spinner&quot; in the app. If the system was under heavy load some of the services involved in the orchestration process could take a long time to respond. This would mean that the user would be stuck with the spinner in the app not knowing what was going on. The solution was to not block the user for the duration of the orchestration process, but instead return immediately when the backend had received the order request from the user and show an &quot;in progress&quot; item in the app that would change state asynchronously when the backend process had ended.</p>\n<p>The initial design resulted in very bad user experience. It was also the cause of a lot of run due to users inputting duplicate orders, since users would eventually lose patience with the spinner, restart the app and redo their order not knowing the original order was still getting processed.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<p>Another simple trick is to prefer asynchronous over synchronous communication. Is there a user waiting at the other end? If not, perhaps you should consider async request/response and event driven communication instead of synchronous communication. When a distributed system built on synchronous communication comes under heavy load it becomes very unpredictable. As resources are running out (cpu, memory, disk, caches and internal buffers spilling over etc.) the various services will start failing, perhaps only partially, become unresponsive or just slow, and this can have a cascading effect throughout the distributed system. This could result in a lot of run work. A really good source of examples on this is in the excellent book <a href=\"https://www.amazon.com/Release-Production-Ready-Software-Pragmatic-Programmers/dp/0978739213\">Release it</a> by Michael Nygaard.</p>\n<h2 id=\"non-functional-requirements\" tabindex=\"-1\">Non-functional requirements <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/fail_fast/\">#</a></h2>\n<p>One obvious way to reduce run work is to be tolerant about failures/degradation. When considering the non-functional requirements during the development phase be critical/realistic about need-to-have and nice-to-have. Make strict alerting on the need-to-haves but be more relaxed about nice-to-have. Perhaps you could make use of SLIs and SLOs to keep track of the service levels in general and react to those rather than individual incidents.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b></b></p>\n<div style=\"margin:2em\"><p></p>\n<p>Some time ago I was working on an ingestion pipeline that had two separate parts. One part was responsible for ingesting the raw data into a datalake and the second part was responsible for applying a schema to the raw data and making it available for consumption in a data warehouse. If data was lost in the ingestion pipeline that ingested the raw data it was very hard or even impossible to replay the lost data from the source so it was absolutely critical to avoid data loss. In contrary, applying the schema to the raw data from the data lake was a repeatable process, if it failed it could be run again against the raw data from the datalake, and the consequence of any issues preventing the job from running would be delayed data delivery and not data loss. As a natural consequence of this difference we focused on making the ingestion of the raw data as robust as possible (implementing at-least-once delivery guarantee) and ensured that observability was in place even before going to production.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h2 id=\"ops-driven-design\" tabindex=\"-1\">Ops driven design <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/fail_fast/\">#</a></h2>\n<p>Let's say you're building a backend. How do you recover when some bug occurs and you accidentally called a service with some wrong data, how do you compensate for the mistake? Does it require you to make code changes or is it possible to do the compensating action by running a script? How do you clean up any bad state in the system databases or external systems caused by the incident? Again, how long does the recovery take? And how much time will you as an engineer spend on this? See <a href=\"https://example.com/dev-blog/blog/sagas/\">sagas</a> for an example on how we reduced run work by introducing a saga framework that allowed us to make generic tooling that could be used to control the sagas in production.</p>\n<p>Let's say you're building a data pipeline. What happens if something unexpected happens and your pipeline stalls? After having fixed the issue can you resume from where it left off? Can you backfill data that was lost due to the incident? Can you backfill just the subset of the data you need or will you need to reprocess everything? How long will the recovery process take? If you have a plan, and the expected recovery time is reasonable, you can worry a little less about making mistakes and perhaps take mores risks. See <a href=\"https://example.com/dev-blog/blog/data_platform/\"> How to reach production in less than 6 months</a> on how I applied this principle on a data platform.</p>\n<p>Design decisions you make early on can have a huge impact on how easy the solution will be to operate once it gets into production, therefore as you design your solution you should think about this from day 1. An analogy to this is code testability. If code is not written with testability in mind adding unit tests later will often require refactoring the code. This is one of the arguments made by TDD. By writing tests in parallel with the implementation this won't happen and your code will be testable (and better designed).</p>\n<p>One way to achieve something akin to TDD could be to keep a staging environment running from the beginning of your development process and try to keep this environment in good state, i.e. if bad things happen you try to resolve any issues and recover the system in a similar way that you will do once you're in production. This way you'll find out if your design and the tooling you have in place is adequate to support the common run work that will occur. Obviously incidents that occur early on in the development process might not be representative to problems that you will see in the more mature system that reaches production so some level of pragmatism would be needed here. It would probably be a good idea to have a &quot;wipe&quot; tool that will restore the state of the system to a good state (perhaps just deleting all data) with a single touch of a magic wand.</p>\n<h2 id=\"alert-strategy\" tabindex=\"-1\">Alert strategy <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/fail_fast/\">#</a></h2>\n<p>Think carefully about your alert strategy. A very common default approach is to set up generic alerts that simply alerts on all kinds of errors, however this can lead to excessive alerting, forcing engineers to waste time manually segregating critical from non-critical alerts. Not only does this lead to alert fatigue it is also a huge waste of time. Investments into optimising this can potentially have a large impact as the overhead of filtering through the alert storm quickly compounds over time.</p>\n<p>A good approach is to alert only on high severity or critical issues, i.e. issues that effect a large proportion of users or which could have fatal consequences if not dealt with immediately. These are the types of issues that demand the immediate attention of engineers and that you would want to be included in an on-call setup.  Everything else could be dealt with using a pull approach rather than a push approach. That way engineers can manage their own time better, do less context switching and be more efficient overall. You could set up a dashboard visualising the non-critical errors as they pile up in your backlog and regular check in on the dashboard to ensure issues are dealt with continuously. E.g. you could build a dashboard in grafana using panels that show up when issues arise:</p>\n<p><picture><source type=\"image/avif\" srcset=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.avif 900w\"><source type=\"image/webp\" srcset=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.webp 900w\"><img alt=\"\" loading=\"lazy\" decoding=\"async\" src=\"https://example.com/dev-blog/img/UUBKq8Cwa4-900.png\" width=\"900\" height=\"294\"></picture></p>\n<h2 id=\"tooling\" tabindex=\"-1\">Tooling <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/fail_fast/\">#</a></h2>\n<p>Do you have the ops tools in place that you need to solve the problems you'll be facing? Obviously you'll need observability, but once you realise that there is a problem how easy is it to diagnose what happened and resolve the issue? Run work will happen, and if you're spending an unreasonable large amount of time logging into various systems, accessing databases, copy pasting data between systems etc. the overhead will compound over time.</p>\n<p>One thing you can do is make it easy to access the various sources of data you need to diagnose and resolve the issues. You could create a tool where you can access and correlate all the data and where all levers that can be pulled are also available. See <a href=\"https://example.com/dev-blog/blog/reducing_run\">Running fast</a> on ideas on how to make such tooling.</p>\n",
			"date_published": "2024-07-04T00:00:00Z"
		}
		,
		{
			"id": "https://example.com/dev-blog/blog/terra_incognita/",
			"url": "https://example.com/dev-blog/blog/terra_incognita/",
			"title": "Terra Incognita",
			"content_html": "<p>In this post I discuss maintainability of systems built around the microservices architecture and try to emphasize the importance of investing into gaining a proper understanding of the problem space. The intended audience of this post are junior/mid level engineers that are new to microservices.</p>\n<p>DISCLAIMER: The topic grew a bit larger than anticipated and the post has become quite lengthy! I'm still iterating on it but I've decided to post it in its current state.</p>\n<h2 id=\"introduction\" tabindex=\"-1\">Introduction <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>There are, at all times, two models at play. There's a model of the problem space, which is a model of the business problem you're trying to solve and a model of the solution space which is your current code base. The model of the problem space may not be explicitly defined. It is often implicit and different people may have a different understanding of it. When the code is written this understanding of the problem space is used by the developers to model the code. It is often quite complicated and driven by the business so there is a high risk that the developers do not have a proper understanding, i.e. they end up implementing something else than what the business expected. A design approach called Domain Driven Design (DDD) has been invented to tackle this common issue by giving the developers a set of tools and design principles to help develop and maintain an explicit model of the domain (the problem space) which is aligned with the understanding of the problem space by the stakeholders and product owners. For example, developers might invite relevant subject matter experts to event storming sessions where they in collaboration come to a common understanding of the business processes and capabilities that define the domain they need to model. They'll develop artifacts, e.g. notes, descriptions, diagrams, wireframes etc. that describe the domain model, i.e. makes it explicit. DDD also contains many best practices that aims at aligning the solution space (the code) as much as possible to the problem space, e.g. by introducing a common &quot;ubiquitous&quot; language which is shared between people from the business and developers and that is used throughout the code base.</p>\n<p>Throughout this blog post I will be using the term <code>bounded context</code>. The term comes from DDD and defines the boundary inside of which the &quot;ubiquitous&quot; language is well defined, i.e. where terms from the language has a single definition. Outside of it a term from the language may have a different meaning. In a microservice architecture the ownership of the solution is usually separated into multiple independent teams, as having the ability to do so is usually one of the primary reasons for choosing a microservice architecture. As each team is independent it follows that teams will not be sharing bounded contexts as that would require the teams to agree on every term used in their codes bases. On the other hand, it is possible for a single team to have multiple bounded contexts, e.g. they could be owning two services that both refer to the term <code>Product</code> but with different meanings. In this text, when the term <code>bounded context</code> is used you can also think &quot;team&quot; as a bounded context originates from a single team.</p>\n<p>I often encounter the attitude that DDD is not worth the effort and in my opinion those developers are underestimating the impact it can have. When developing a distributed backend, I believe that DDD is a vital tool for decomposing the backend into smaller decoupled and autonomous components (e.g. microservices) and ensure clear and stable responsibility boundaries. In my opinion, building a microservice backend without an up front and continuous effort into understanding the problem space, e.g. by employing DDD, is a blatant mistake. A mistake that I see happening too often. Obviously the real world does not allow for idealism. Time to market and budgets demands short term focus rather than long term planning but it is the responsibility of the software developer or architect to make good and conscious decisions and I believe that even small efforts can have a compounding effect that will help scalability and maintainability on the mid to long term. Conversely, neglecting it will have adverse effects, on which I will give examples in the anti-patterns section.</p>\n<p>DDD is a very large topic. This post will not be going into details about DDD, and although some familiarity with DDD concepts is a good prerequisite, the text can be read without prior knowledge, but the reader is encouraged to read up on DDD later. The post will not discuss all benefits and consequences of applying DDD, rather it will focus on a single aspect only, the maintainability/adaptability of the software.</p>\n<p>My intention with this blog post is to increase the awareness of the importance of investing into understanding the problem space. In my opinion it will increase productivity, profitability and contribute to a happier developer life.</p>\n<h2 id=\"solution-maintainability\" tabindex=\"-1\">Solution maintainability <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>The domain model (whether explicit or implicit) is not static. Like the business, it will evolve over time. New or changing requirements will continuously alter the domain model and as it changes the developers will need to evolve the code base to fit the new requirements. The concepts making up the domain model, like actors, entities, processes etc. are all related to each other, some more than others. When following a DDD approach, once all business processes has been sketched out in an event storming session it should stand out how the concepts are coupled to each other. The degree of coupling of all those relationships should be respected in the solution space. If something is closely related in the problem space that should also be the case in the code you write and if they're independent they should not be coupled in the code. This is in fact what is being expressed by Conway's law which you may already be familiar with:</p>\n<p></p><div style=\"margin-left:15%;margin-right:15%; background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.7em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Conway's law</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>Organizations which design systems (in the broad sense used here) are constrained to produce designs which are copies of the communication structures of these organizations.</p>\n<p></p></div>\n<p style=\"margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia\"> <b>- Melvin E. Conway</b> <i>(How Do Committees Invent?)</i>\n</p>\n</span>\n</div><p></p>\n<p>If there's a misalignment between the problem space and the solution space, i.e. your code and the domain model are structured differently, simple changes in the problem space can turn out to be much harder to carry out in the solution space than they should be. To make this point let's turn to some of the quality metrics of software design that you're most likely already familiar with.</p>\n<p>You're probably already familiar with the term <strong>single responsibility</strong> as it's often mentioned as an important quality attribute of software solutions. However, there can be different interpretations of what this concept means, so in order to avoid any confusion let's do a short recap here and establish what the author means when referring to those concepts.</p>\n<p>The <strong>single responsibility principle</strong> states that a component (a class, module, microservice etc.) should have one and only one responsibility. What is meant by this? When does a component have multiple responsibilities?</p>\n<p>Robert C. Martin has <a href=\"https://blog.cleancoder.com/uncle-bob/2014/05/08/SingleReponsibilityPrinciple.html\">expressed the principle</a> as <strong>a class should have only one reason to change</strong>. Another way to express this is to say that in order to comply with the single responsibility principle code that changes for the same reason should be close together (coupled) where as code that changes for different reasons should not.</p>\n<p></p><div style=\"margin-left:15%;margin-right:15%; background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<div style=\"margin:2em\"><p></p>\n<p>If you think about this you’ll realize that this is just another way to define cohesion and coupling. We want to increase the cohesion between things that change for the same reasons, and we want to decrease the coupling between those things that change for different reasons.</p>\n<p></p></div>\n<p style=\"margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia\"> <b>- Robert C. Martin</b> <i>(The Clean Code Blog)</i>\n</p>\n</div><p></p>\n<p>So what is a &quot;reason to change&quot; the code?</p>\n<p>The reason to change the code comes from the stakeholders that operate in the problem space. They want new or improved features. The stakeholders are coming from the business and are organised using the business org chart. E.g. at Amazon there could be a stakeholder Aaron from the &quot;product&quot; department and another stakeholder Bridget from the &quot;checkout&quot; department. Aaron is concerned with how products are organised and presented on the Amazon site, Bridget is concerned with the user experience after a purchase decision has been made, e.g. what does the email receipt after the purchase look like? So if Aaron asked you to make a change to how product information is presented and you accidentally also changed how it was shown on the email receipt because the component you altered was being used in both places (i.e. it was responsible for delivering product information to both) then your component does not have a single responsibility. The product page and the email receipt are independent concepts in the problem space because the business has been organised in such a way that they are the responsibility of different departments that each have their own stakeholders. For that reason they will probably change independently and for different reasons. If Aaron wants the product information to look differently there is no reason to expect that Bridget will want the same change or even be aware that Aaron has requested the change.</p>\n<p>So in this case the component was responsible for business logic that is coupled to multiple independent concepts in the problem space. The concepts are decoupled in the problem space but they were coupled in the solution space.</p>\n<p></p><div style=\"margin-left:15%;margin-right:15%; background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<div style=\"margin:2em\"><p></p>\n<p>However, as you think about this principle, remember that the reasons for change are people. It is people who request changes. And you don’t want to confuse those people, or yourself, by mixing together the code that many different people care about for different reasons.</p>\n<p></p></div>\n<p style=\"margin-bottom:1em; margin-right:1em; text-align:right; font-family:Georgia\"> <b>- Robert C. Martin</b> <i>(The Clean Code Blog)</i>\n</p>\n</div><p></p>\n<p>Having this misalignment of the problem space and solution space means that changing your solution will be hard. As the problem space evolves, changes to different parts of the problem space (which are independent and decoupled) will be difficult because of the entangled mess. A change which is simple to express in the problem space will be very hard to do in the solution space because the component responsible for the concept is coupled to other concepts from the problem space.</p>\n<p>You can experience the inverse problem as well. Your component shares its responsibilities with other components. Those other components might even be owned by other teams. So when Aaron wants his change you'll need to make changes in multiple places in order for the change to be implemented. If multiple teams are owning the components this means coordination, meetings, project management etc. which could otherwise have been avoided. This is what is referred to as low cohesion in any <code>introduction to programming</code> text book, and it is an undesirable characteristic.</p>\n<p>So this is really all about cohesion and coupling. You want high cohesion, behaviour that is related should be close together, and inversely behaviour/features that are unrelated should not be close together in the code, that is, have low coupling.</p>\n<p>The crucial point here is that you cannot comply with the single responsibility principle (or more generally keeping cohesion and coupling in the code just right) unless you have a proper understanding of the problem space because the changes to the code (solution space) are driven by changes in the problem space. In other words things that change for the same reason do so because they're related in the problem space, and things that change for different reasons (and should therefore not be coupled in the code) do so because they're independent concepts in the problem space. So if you don't have a proper understanding of how concepts are related in the problem space how could you possibly make good decisions on how to structure your code in such a way that you don't break the single responsibility principle?</p>\n<h2 id=\"responsibility-boundaries\" tabindex=\"-1\">Responsibility boundaries <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>When a system based on a microservice architecture is implemented without a proper understanding of the problem space it will result in accidental complexity, unintentional coupling and a tightly coupled architecture in general. This has many negative consequences but, as mentioned, in this post we will focus on maintainability. When the misalignment is allowed to exist you'll often be able to notice the consequences by observing the responsibility boundaries that exist between the bounded contexts in your backend. Some bounded contexts will have multiple responsibilities, some bounded contexts will share responsibilities with each other, and some boundaries will simply be unclear.</p>\n<p>As mentioned earlier, in this post, we'll be focusing on challenges that arise when building systems using a microservice architecture. From the perspective of the single responsibility principle a microservice is just like any other component (e.g. classes or modules). However, microservice boundaries are a lot more rigid because they involve network and often team boundaries. This means that the single responsibility principle becomes even more important, because once you break it, it is much harder to refactor and correct the issue.</p>\n<p><em>In the rest of the post I will be assuming that teams are organised as stream-aligned/vertical teams. I.e. teams that has the power to take full stack domain ownership. If teams are instead organised in layers, e.g. a database team, a service team and a front-end team, it changes the discussion.</em></p>\n<p>There are three common cases of bad responsibility boundaries that you'll encounter, let's go through them here.</p>\n<h3 id=\"violation-of-the-single-responsibility-principle\" tabindex=\"-1\">Violation of the single responsibility principle <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p><strong>A single component has the responsibility of multiple independent business capabilities or processes from the problem space.</strong></p>\n<p>Because the implementation of the business capabilities are entangled in the code it will be difficult to evolve them independently. This can happen on many levels, it could be an aggregate inside a service taking up too much responsibility, it could be a service or a bounded context.</p>\n<h3 id=\"shared-responsiblity\" tabindex=\"-1\">Shared responsiblity <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p><strong>The implementation of a single responsibility from the problem space is scattered between two or more independent components in the solution space.</strong></p>\n<p>In a microservice architecture the components could be two bounded contexts owned by separate teams.</p>\n<p>From the perspective of the owners this means that implementing a change request is hard because it requires the coordination of multiple teams. They must align roadmaps and all teams will depend on each other while implementing and testing the new changes.</p>\n<p>From the perspective of dependent teams that must implement business rules or processes that are based on entities or events coming from the domain it will be difficult to implement and maintain their solutions. Since domain events are coming from multiple bounded contexts how do they know that they have consumed all relevant events, and how can they be sure which events are relevant, the publishers might use different names for the same thing, or the same name for different things. Likewise how could they ensure that their solution will continue to subscribe to all events in the future, if ownership is already scattered, what's to stop yet another team from publishing new events belonging to the domain in the future? Similar problems arise when relying on scattered APIs.</p>\n<h3 id=\"inconsistent-responsibility-boundaries\" tabindex=\"-1\">Inconsistent responsibility boundaries <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p><strong>There are clear responsibility boundaries but they are drawn inconsistently across another orthogonal dimension.</strong></p>\n<p>A Business capability is owned by a single team A, except for the cases where the users logged in are coming from country C, in those cases the business capability is owned by team B.</p>\n<p>This will obviously make it harder to evolve the business capability because both team A and team B will need to be involved, at least if feature parity is a concern. It will also likely cause a lot of duplicated effort.</p>\n<h2 id=\"anti-patterns\" tabindex=\"-1\">Anti-patterns <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>In the next section we will look at common cases of bad decision making that reduce the maintainability of the system, but first, let's examine a set of common anti-patterns related to maintainability that are frequently observed in systems where the understanding of the problem space has been lacking. The list is not exhaustive.</p>\n<h3 id=\"anti-pattern-1-bad-fit\" tabindex=\"-1\">Anti-pattern 1: Bad fit <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>As the problem space evolves it is natural to consider reusing or extending existing components to support the new business processes/capabilities. It might be possible to support the new requirements using an existing component by making minor extensions, e.g. by making functionality more generic and configurable or adding branching logic. If one does not consider whether the new business capability/process is part of the same domain as the existing functionality this can lead to <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Violation of the single responsibility principle</a> and <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">misaligned abstraction</a>.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Real world example</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>A team was responsible for a core domain that was depended on by many other domains. Those other domains represented the products delivered to the users of the system. The company was using stream aligned teams (i.e. teams with vertical/fullstack ownership) and the team could in the terminology used in team topologies be characterised as a complicated subsystems team. However, the team was also itself using the core domain to implement a product and was thus also acting as a stream aligned team.\nThe team had built a BFF to enable the front-end of their product. The BFF exposed an API, used by the mobile app, that would authenticate the user and then delegate to the underlying backend service. It was also responsible for how the product was shown in the UI and for sending out notifications to users and this was implemented in an event driven way, by consuming event messages published by the backend service and then reactively refreshing the UI or pushing notifications.</p>\n<p>The first few products built by other teams had no special requirements with regards to the UI and notifications, and it was therefore decided that they could be relying on this BFF to handle that part of the functionality for them. Later products had slightly different requirements. E.g. one product needed to show a different text in the notifications and that functionality was therefore implemented in the service belonging to that product and a corresponding <code>if</code> statement was added to the BFF to exclude the notifications for that specific service. The product also had one difference in the functionality in the UI, the item shown in the UI could not be deleted for this product which was possible in the common functionality, thus another <code>if</code> statement was added here. Another product did not exist in the app at all, it was provided to a different user base that did not use the app. Again, appropriate <code>if</code> statements were added in the BFF to exclude the handling of the UI and notifications for this product.</p>\n<p>Do you see where this is going? The BFF service broke the single responsibility principle, it implemented common functionality for products that were not related. Every time new products were added, or existing products were altered, one needed to consider whether that product feature set would fit into the BFF's common functionality and if not make special cases to exclude certain parts. Likewise when making changes to the functionality within the BFF service a developer would need to consider the entire set of products that was using the BFF service to ensure that the changes made did not break any existing functionality in any of those products. This was a near impossible task as every product was different and the product catalog spanned many domains. To make matters worse, the list of products using the BFF was not readily visible in the code but could only be obtained by asking developers that had the historical context or looking at call graphs or logs. The initial intention of being more efficient by reusing existing functionality so that product teams did not have to write code that already existed eventually led to a brittle and hard to maintain system.</p>\n<p>Investigating the domain should have made it apparent that current and future products were not identical. Also, since the products were user facing, as time goes by it is only natural that requirements for each product diverges because product owners will want to iterate on their products making them more sophisticated. If this had been realised from the beginning developers could have chosen to either not reuse any functionality at all or, if feasible, put reusable parts of the code into a library that the other product teams could use to implement their product features.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h3 id=\"anti-pattern-2-ghost-domain\" tabindex=\"-1\">Anti-pattern 2: Ghost domain <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>As the business evolves and is expanded with new capabilities/processes new domains can emerge. If the new domain is not identified in time, implementation of the related capabilities/process could get scattered all over the organisation and ending up in multiple bounded contexts leading to <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Shared responsibility</a>.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Real world example</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>A company would charge fees for the different products/services they provided to their customers. There were many products/services and they were implemented by the various microservices in their microservice architecture and initially each microservice would directly charge a fee if they had carried out an action that required it. After some time it became apparent that there were many business rules and processes related to the management of fees. Some types of fees should only be charged if the user was in a certain membership tier, there were rules about discounts, e.g. if many fees had been charged in a short period. The company was also required by law to send out yearly fee statements to their customers so they could see which fees had been charged. Implementing these business processes quickly became a nightmare due to the scattered implementation of the fees. While some of the requirements related to the fee domain might not have been known initially, the requirement dictated by law should have been anticipated and would naturally have let to the discovery of the fee domain early in the development process.</p>\n<p>The solution was to specify different types of fees and have each microservice notify the fee service about fees that needed to be charged but let the fee service take responsibility for handling the fees. This kept the decision on which specific action required fees to be charged within the service that implemented them, ensuring a low coupling of the fee service to the other services while keeping the responsibility of handling the fees and all related business processes within the fee service.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h3 id=\"anti-pattern-3-entanglement\" tabindex=\"-1\">Anti-pattern 3: Entanglement <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p><strong>The representation of the problem space in the code is entangled with technical concepts from the solution space.</strong></p>\n<p>This means that when the solution changes for purely technical reasons, e.g. because of a non functional requirement, code containing business logic will likely need to be refactored. This is wasteful and, as always, comes with the risk of introducing bugs. If the solution specific concepts has bled into the APIs or events this could also effect dependent services.</p>\n<p>Business logic from the domain model leaks into technical components or technical components leak into the domain model. This violation of separation of concerns means that when you need to change your code for technical reasons (i.e. not to change business logic), e.g. fixing a bug, upgrading a library/framework to a newer version or simply refactoring the code you risk making unintended changes to the business logic. Likewise, understanding the business logic that comprises the domain model becomes harder because the code is entangled with code that is only there for technical reasons and does not exist in the problem space.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Real world example</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>A team was responsible for modelling a <code>SalesOrder</code> entity. As part of managing the SalesOrder process the team was also doing a lot of orchestration, calling other services and updating the state of the SalesOrder entity accordingly. In this particular case the orchestration process was purely there to implement a distributed transaction and did not represent a business workflow and as such only lived in the solution space. However, they had not separated the orchestration process from the modelling of the SalesOrder entity and this resulted in details related to the orchestration process to leak into the domain model. The domain model was responsible for storing information such as &quot;async request to service X has been initiated, waiting for a response&quot;. Whenever a change to the orchestration process was needed, e.g. a service that was depended upon was replaced with another, this meant touching code that was located in the same place as the domain model, and therefore also risking making unintended changes. Sometimes the orchestration process (saga) would get into a bad state because something unexpected happened and it was needed to get it back on track. This was done by introducing new steps in the process that were only there because of the specific incident that caused the saga to end up in the bad state. Over time this polluted the saga code, and therefore also the domain model code with logic that made very little sense without the context of the historic incident, leading to a bloated domain model that was hard to understand and maintain. The sagas in this example were breaking the single responsibility principle. They were responsible for implementing the business process AND for implementing the orchestration.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<p>As an example of entanglement consider a service that publishes an event with information needed for a salesforce integration, perhaps the event contains a salesforce related ID. The event is being used as a domain event also. When the partnership with salesforce ends it will not be possible to stop publishing the event because it is used elsewhere.</p>\n<p>Another example of this could be a single concept (e.g. a product catalog) being spread out over multiple event definitions, because they happen to be published by two different microservices in the solution space. This will make it very hard to consume the data. How do I know that I have consumed all the data related to product catalogs? How do I know whether this business rule that I must implement is taking all instances of an enitity into consideration if events pertaining to that entity are being published in many places in the backend, and how do I know that yet another event will not be created in the future thus breaking the correctness of the implementation because it will be ignorant of this new event? It will also be brittle, because when someone wants to refactor the backend for technical reasons (not because of changes to the problem space), e.g. merging two microservices into one, the event definitions might change causing dependent teams extra work to change how they consume the data. This would not have happened if they were proper domain events because proper domain events will only need to change when there are changes in the problem space.</p>\n<h3 id=\"anti-pattern-4-misaligned-abstraction\" tabindex=\"-1\">Anti-pattern 4: Misaligned abstraction <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p><strong>A bounded context has implemented an abstraction which does not exist in the problem space. Changes to requirements breaks the abstraction forcing the owning team and all dependent teams to refactor.</strong></p>\n<p>An important aspect of modelling is abstractions. Developers come up with abstractions that will represent some concept or reusable functionality. What happens if the abstractions are not grounded in the problem space? Remember, the requirements are born in the problem space and they will tickle down and hit the abstraction, but if the abstraction is not something that exists in the problem space there is no guarantee that these requirements will align with it. The addition of a new requirement might not fit within the abstraction, forcing the developer to rethink the abstraction or create another. It is therefore a good idea to think about your abstractions from the perspective of the problem space. Did you come up with the abstraction yourself while coding or do the business stakeholders know what this is when you mention its name? Is it something that exist in some shape or form in the problem space, either concretely as a physical object or something less tangible like a concept people from the problem space are using in conversations. As always, a stint of pragmatism is good, your abstractions need not all be grounded in the problem space, but it is always a good idea to be conscious about these things and accepting the risks that comes with your misaligned abstractions.</p>\n<h3 id=\"anti-pattern-5-dominant-partner\" tabindex=\"-1\">Anti-pattern 5: Dominant partner <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>A system integrates with a vendor or partner that provides capabilities to the company that spans multiple of the company's domains and this violation of single responsibility bleeds into the architecture of the system. In this scenario a single team is given the responsibility of integrating with the vendor/partner and that single team implements all the business processes that are related to those capabilities. That team now has responsibilities that are belonging to all those domains. Depending on the size of those domains this can end up being too much responsibility for a single team to handle. As the company evolves and expands into new teams that will take over some of those domains, the responsibility of the existing code will need to be handed over, otherwise the initial team will now have a slice of the responsibility of all those domains, leading to a shared responsibility with the other teams. If the initial team had segregated the code belonging to the different domains into separate microservices handing over the code is a matter of handing over the responsibility of those service to the respective squads. However if the initial team had coupled together the code from the various domains, untangling it and handing it over can be a sizeable task, most likely something that will be postponed. This leads to <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Violation of the single responsibility principle</a>, <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Shared responsibility</a> and <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Inconsistent responsibility boundaries</a>.</p>\n<p>This situation often occur because developers are not even aware that multiple domains are involved. It can also occur because the ramifications of ending up in this situation is greatly underestimated.</p>\n<p>It is very similar to the well known Blob (or God object) antipattern. The team and their code is ubiquitous. The team members are required in most meetings and your architect will go around the office and utter sentences such as &quot;this team is the beating heart of the company&quot;, and will likely not realise that this is a very bad thing. Worse still, those team members will become highly esteemed and have a high degree of influence, a situation that can be desirable for an engineer and there is a risk that those engineers will not be motivated to push for changes.</p>\n<h3 id=\"anti-pattern-6-leaking-complexity\" tabindex=\"-1\">Anti-pattern 6: Leaking complexity <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>While having a good understanding of your own domain is paramount it is not sufficient. In order to build good APIs and good integrations with other bounded contexts it is necessary to have some understanding of the business processes that are depending on your domain. Having this understanding enables you to design APIs that are fit for purpose. Not having this understanding usually leads to teams building APIs that reflect their domain model directly. If the domain is a complex one this will force dependent teams to understand and interact with the domain model in its full complexity even if their use case is a simple one. E.g. in an event driven architecture external events would reflect internal domain events directly and in a complex domain there would be many events and many details and relations to understand and writing and maintaining the code that consumes the data can be a big task.</p>\n<h2 id=\"project-management\" tabindex=\"-1\">Project management <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>In organisations characterised by short term focus and where activities concerned with gaining an understanding of the problem space are not valued or prioritised project management will often take decisions that will make the situation worse. In this section I've listed a few examples.</p>\n<h3 id=\"accumulating-tech-debt\" tabindex=\"-1\">Accumulating tech debt <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>As the business expands some teams will no longer be able to deal with the cognitive load and responsibility boundaries will need to be altered. Sometimes this is done by placing new feature development with a different team and leaving the old team with the maintenance of the old solution. Often there is no agreed plan on when and how to migrate the old responsibilities to the new team and this results in a fragmented solution space that suffers from e.g. <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Inconsistent responsibility boundaries</a>. Quite often the reason for not prioritizing the alignment is that the consequences of not doing so are underestimated.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Real world example</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>A company entered a new country and needed to generalise the existing features to work in the new country but also build new country specific features. The  responsibilities for the existing solution were split between a stream-aligned team and a <code>complicated subsystems</code> team. When development started on adding support for the new country it was decided that it would be better for the complicated subsystems team to be a stream-aligned team, and a new solution based on this decision was built for the new country. Years passed, and the misalignment of the architectures for the different countries persisted. In the new country the team acted as a stream-aligned team and in the old country that role was still assigned to the old team. This caused confusion for the developers and all stakeholders. E.g. Triaging incoming service desk tickets was a big challenge because it was quite difficult for non-technical staff to determine ownership of specific tickets.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h3 id=\"responsibility-is-delegated-based-on-roadmap-bandwidth\" tabindex=\"-1\">Responsibility is delegated based on roadmap bandwidth <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>In an organisation that has no clear idea of the domain landscape in their problem space and where this is not taken into consideration from project management and leadership responsibility of new projects are often delegated based on team availability. This is the most obvious way to delegate tasks as it optimises utilisation of manpower, but there is obviously no correlation between roadmap bandwidth and domain ownership, so this naturally leads to a scattered ownership landscape, e.g. <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Inconsistent responsibility boundaries</a>. There is nothing inherently wrong in taking roadmap bandwidth into consideration but if there is no anchoring in domain ownership to help steer the decisions it quickly leads to entropy.</p>\n<p></p><div style=\"background-color:#26394D; color: #c3c3c3; border-left: solid #37526F 4px; border-radius: 4px; padding:0.3em;\">\n<span>\n<p style=\"margin-top:1em; text-align:center\"><b>Real world example</b></p>\n<div style=\"margin:2em\"><p></p>\n<p>A team was responsible for a domain but only for the users located in some of the countries. For users from other countries the responsibility for the domain lied within a different team, even though the problem space (in this example) did not distinguish between users from different countries. This situation can e.g. occur when different countries requires different integrations and the implementation of those integrations are carried out over time correlated to when the company enters those countries. In those situations there is a possibility that the team that owns the domain does not have room in their roadmap when the time comes and the implementation task is then handed over to another team. This leads to the domain responsibility being scattered. Since, in this example, the country boundary does not exist in the domain, product evolution will happen across this boundary going forward, i.e. new features will/should be introduced and changed for all countries at the same time, however since the responsibility for all countries are not owned by a single team, implementing the new functionality would involve multiple teams leading to project and roadmap coordination work and possibly duplicated effort for the implementation.</p>\n<p></p></div>\n<p></p>\n</span>\n</div><p></p>\n<h3 id=\"responsibility-is-delegated-based-on-technology-familiarity\" tabindex=\"-1\">Responsibility is delegated based on technology familiarity <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>When the implementation of a new business capability involves a specific technology already known by a subset of teams one of those teams will be selected for carrying out the implementation while completely disregarding if the new business capability belongs to a domain that the team is already responsible for. This leads to <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Shared responsibility</a> or  <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Inconsistent responsibility boundaries</a>. Responsibilities of new capabilities that belong to domains owned by other teams wíll get assigned to the team that has the experience with the technology and the team is effectively taking slices of the responsibilities that should belong to the other teams.</p>\n<p>It is not necessary for technology based knowledge to be tied to specific teams. The experienced team can act as mentors and share knowledge with other teams, or they can build generic (i.e. not domain specific) components, like libraries or services, that can be used by other teams that need to use the technology.</p>\n<h3 id=\"responsibility-is-delegated-based-on-solution-space\" tabindex=\"-1\">Responsibility is delegated based on solution space <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>A component of the system is taking on too much responsibility and is often involved when new features are developed. For this reason the team owning the component will often be delegated responsibility of business processes/capabilities that belong to domains owned by other teams. This is a variant of the dominant partner antipattern, in this case though, the system that crosses responsibility boundaries is built in-house but the effects are the same. This situation often occur because developers are not even aware that multiple domains are involved. It can also occur because the ramifications of ending up in this situation is greatly underestimated.</p>\n<p>This is often seen when complex subsystem teams or platform teams builds generic components that gets entangled with the business domains. E.g. it could be a team owning a CRM product or a customer service product. If care is not taken to allow other teams to plug in to the generic component often the team owning the generic component ends up becoming a bottleneck and ends up taking slices of the domains of other teams, leading to <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Shared responsibility</a> or  <a href=\"https://example.com/dev-blog/blog/terra_incognita/\">Inconsistent responsibility boundaries</a>.</p>\n<h2 id=\"why-is-this-so-hard\" tabindex=\"-1\">Why is this so hard <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>This is all really quite obvious, isn't it? You need to understand the problem you're solving. So why is it so hard to get right?</p>\n<h3 id=\"dry\" tabindex=\"-1\">DRY <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>Let's talk a bit about the DRY principle, don't repeat yourself. If code is duplicated, maintenance becomes difficult. When making changes you need to remember to update the code in all the places it's duplicated and there's a chance you'll miss some of it, leading to inconsistent code and bugs! Therefore we should move the duplicated code into a function or a class and then reference that. As a developer this is one of the first principles you learn. It is easy to grasp and makes so much sense! This is really what programming is all about, creating reusable functions and classes that can be composed into larger structures. Even your IDE reminds you about this, if you have duplicated code it will tell you. The duplicated code is staring you right in the face! Just remember one thing, the DRY principle is defined within the confines of your solution space. What if that seemingly duplicated code is actually representing two different concepts in the problem space that, for the time being, just happens to have the same definition in the solution space? Think about the product catalog and the checkout domains from the Amazon example. They both have a product entity and they have the same definition but when Aaron asks you to add a &quot;related products&quot; property to the product catalog and Bridget does not, they are no longer the same. If you understand the problem space you also understand that the product catalog and the checkout process are two very different concepts and you will realise that when you look at your code. You'll know that they'll likely diverge in the future and will resist the temptation to apply the DRY principle and merge together the two identical product classes into one. In this example it is very obvious that there are two representations of a product, but your particular problem space might be much more complex or a lot less intuitive than this toy example. If you don't understand the problem space and is not constantly aware of its presence you'll likely miss this and make the obvious choice and apply the DRY principle instead.</p>\n<p>The DRY principle only makes sense to adhere to if the duplicated code is duplicating concepts from your problem space. Let's say you had implemented a business capability in a service and you needed to support another capability that had the same functionality up to that point but where the remaining work would diverge, then there is nothing fundamentally wrong about copying a service consisting of 1000s of lines of code and handing it over to another team to allow them to build that new capability while allowing you to evolve the existing. This doesn't happen often in practice, but consider it. It goes against your gut to copy 1000s of lines of code, but your gut feeling could be wrong.</p>\n<p>Sometimes pragmatism wins and reusing a service even if it is not fit for purpose is still the right choice. Time to market, resource allocation constraints etc. should of course always be factored in. However, there is a big difference between making this choice with open eyes, knowing that this could cause problems later as the solution evolves and taking necessary precautions, and on the other hand not realising that there even is an issue and end up in a bad situation later when the solution is not fit for the new requirements.</p>\n<h3 id=\"deceptiveness\" tabindex=\"-1\">Deceptiveness <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>You can spend your entire professional life confined within the solution space, adapting the solution iteratively as you learn more about the requirements during development. You can skip the event storming, the ubiquitous language and all of the other DDD practices. It will work and you'll be able to deliver the requested feature. Some new requirements will require a larger overhaul of the design because they cannot easily be adapted, the project will last several months/year but you'll eventually deliver the new features, while also having improved and cleaned up the solution, and everyone is happy. Little do the developers and stakeholders know that if only the solution had been a better fit to the problem space to begin with the larger refactoring project would not have been needed and the features could have been delivered in a few weeks. This is an alternative reality that never existed. Stakeholders usually don't have a good idea about what would be a reasonable amount of time to implement a feature. Sure it looks easy on paper, but IT is complex!</p>\n<p>The misalignment between the problem space and solution space is not apparent when a developer is looking at the code. Even if the code looks SOLID it might very well be that the apparent single responsibility of a class in the code is actually multiple responsibilities in the problem space. Later when a change arrives in the problem space it can be hard to implement because the class implementing the related functionality is used to solve a different independent requirement in the problem space.</p>\n<p>As mentioned, when your code base does not have the high cohesion/low coupling characteristics maintainability and evolution of the solution will be harder. However, in a distributed system the evolution of the solution might happen in another team that will then suffer the consequences of the bad choices made by your team. This is very common. This means that you will not yourself suffer the consequences of your bad choices. This can mean that teams do not learn from their mistakes. The price is paid in slower development and a less robust solution throughout the organisation, but it is not at all easy to see the cause and effect from a local perspective. Therefore architects with more of an overview are needed for sparring.</p>\n<p>People think they can take an iterative just-in-time approach with regards to requirement gathering (i.e domain understanding). They have an agile mindset and they think they can POC it, put it in production, gather feedback and then continuously re-iterate. There is nothing wrong with being agile but they don't realise the huge costs associated with rewriting the software. This is especially true for microservice architectures. If the refactoring goes beyond the boundaries of bounded contexts this means changing APIs and coordinating the new release with all involved teams. Perhaps even migrating existing state between services. If the architecture is event oriented it might also involve recreating the new events for historic data and reconstituting to all consumers.</p>\n<p>It is very difficult to detect the misalignment in realtime because quite often the consequences of the misinformed decisions do not appear until later when the solution must be evolved and it is no longer a good fit. By the time that the symptoms, such as a tightly coupled architecture, unclear responsibility boundaries, the constant need for refactoring projects and the build up of legacy code becomes visible, it is far too late.</p>\n<h3 id=\"the-business-is-not-run-by-tech\" tabindex=\"-1\">The business is not run by tech <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h3>\n<p>There is often a big pressure from management to focus efforts on feature development and it can be a challenge to communicate the value of working on other tasks because they are often very technical in nature. This is a well known problem when dealing with e.g tech debt and it is also problem here.</p>\n<h2 id=\"how-to-fix-this-issue\" tabindex=\"-1\">How to fix this issue? <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>As a first measure you should try to assess your situation. If you're not currently following DDD or similar practices chances are that your solution is misaligned with the problem space. Take notice of people from the organisation speaking of areas of responsibilities that you were not aware of. &quot;I really want X (person or group of people) to take responsibility for Y&quot;. If this does not align with your reality then this is an indication that their view of the world is different than yours, that is, your understanding of the problem space conflicts with theirs. In this case your solution might be fitted well to your understanding of the problem space, but that understanding could be wrong or just not aligned with the rest of the organisation.</p>\n<p>Everytime you need to do a bug fix or refactoring task, think about how you ended up in this situation. Did it happen due to not putting enough consideration/effort into the original solution, e.g. because of time constraints, or was the original solution actually thorough, but the issue arose because of incomplete understanding of the problem that was being solved? If the latter case occurs often it is time to put more focus on practices aimed at improving your understanding of the problem space in due time (i.e. not just-in-time)</p>\n<p>Secondly, the business or management might need some convincing before they will allow you to invest resources into improving your understanding of the problem space. The responsibility for doing this usually belongs to a head-of-tech or a CTO if such a role exists in your company. Try to give examples of projects from the past that were harder and took longer than they should have. Projects that were simple in the problem space but became very complex in the solution space. Perhaps you can find examples of projects that involved many teams and required coordination and project management and were difficult to roll out and test due to all the interdependencies even though the problem belonged to a single domain? Being able to deliver projects faster and give more reliable estimates constitute convincing arguments. You could also try to highlight the amount of legacy code that has been generated.</p>\n<p>Finally you might need to make adjustments to the organisation. Every domain should have a designated product owner who has the responsibility of keeping the understanding of the problem space up to date. That involves understanding how the business currently operates and how it is expected to evolve in the near or mid term future. The company should have designated architects. The architects will be in close contact with the stakeholders and product owners and keep an up to date model of the current and future domain landscape. They will understand the current architecture of the solution and be responsible for a target architecture that should strive to align the solution as much as possible with the domain landscape.</p>\n<h2 id=\"final-thoughts\" tabindex=\"-1\">Final thoughts <a class=\"header-anchor\" href=\"https://example.com/dev-blog/blog/terra_incognita/\">#</a></h2>\n<p>Software design is an inherent iterative process. Most of the industry have now moved on from the days of the waterfall process and learned that splitting the process into a design and an implementation phase is almost never feasible. The devil is in the details and even small development tasks can't be designed up front because often new details and unforeseen obstacles appear while coding. However, the fact that development is carried out in an agile fashion should not lead one to believe that gathering requirements and understanding the problem space can wait till the very last moment. The misalignment between the problem space and the solution space that this approach allows to exist will negatively impact maintainability and thus profitability in the long run.</p>\n<p>You can build your design and abstractions based on your intuitions and, depending on how well you know the domain, they'll probably fit well enough, at least for some time, but if the product lives long enough, sooner or later it will catch up to you and the design overhaul will become a reality. This happens all the time. How many of the projects that you worked on were brownfield or greenfield projects? They're mostly greenfield on paper, but many of them are just a new generation of the old solution that no longer was able to fit the problem space. (there are of course other reasons why projects needs to be rebuilt, like deprecated tech stacks, mergers etc.) Again, this is not a big tragedy, but it certainly is very wasteful and not at all cost efficient in the long run. Thing is, budgets are often short lived and time to market is vital. This is often the reality and those factors should of course be weighed in when considering if and how DDD could be relevant. Just remember that projects tend to stay around much longer than the initial budget and if you intend to stay around as well you'll do your future self a huge disservice if you don't think ahead.</p>\n",
			"date_published": "2024-06-21T00:00:00Z"
		}
		
	]
}
